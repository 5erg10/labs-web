[
    {
        "title": "A la conquista del espacio",
        "pubDate": "2018-11-29 13:56:40",
        "link": "https://labs.beeva.com/a-la-conquista-del-espacio-29f1b21c6dbc?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/29f1b21c6dbc",
        "author": "Jorge Andrés Martínez | BEEVA",
        "thumbnail": "https://cdn-images-1.medium.com/max/730/1*tHL1Mgy3x5JBkPUpIcilPQ.png",
        "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*tHL1Mgy3x5JBkPUpIcilPQ.png\"></figure><p>Interacción en espacios físicos, el gran reto de la HCI…</p>\n<p>En su conquista del espacio y de retos impensables (impensables solo para una generación anterior, claro), la tecnología está llevando a nuestro día a día experiencias que hasta hace poco apenas podríamos imaginar. Una de las posibilidades que más nos emociona actualmente es la llegada de <strong>la experiencia digital al mundo físico</strong>, algo que supone multitud de implicaciones que sólo estamos empezando a explorar.</p>\n<blockquote>En esta recién estrenada frontera, la que supone la <strong>conquista de los espacios físicos</strong> por el mundo digital, pensamos que el reto será conseguir experiencias satisfactorias que aporten valor para el usuario.</blockquote>\n<p>Y las cosas avanzan rápido: ya se están ampliando cada vez más las posibilidades de interacción <strong>más allá de las pantallas y las interfaces gráficas “clásicas” y </strong>gracias a esta expansión <strong>se están creando sistemas nuevos y una forma diferente en la que las personas interactuamos con los dispositivos</strong>, algo que supone multitud de retos a los que nos enfrentamos quienes trabajamos en un laboratorio de HCI.</p>\n<h4><strong>Buscando las respuestas</strong></h4>\n<p>Y como no puede ser de otra manera, en esta tarea exploratoria en la que parece que todo es posible, a los investigadores HCI nos asaltan preguntas continuamente, del tipo:</p>\n<blockquote><strong>¿Cómo nos comportamos ante coches y otros dispositivos autónomos?</strong></blockquote>\n<blockquote><strong>¿Como interactuamos con los asistentes de voz?</strong></blockquote>\n<blockquote><strong>¿Está la tecnología deshumanizada?</strong></blockquote>\n<blockquote><strong>¿Cómo está afectando esta nueva frontera de la inteligencia artificial a nuestra privacidad?</strong></blockquote>\n<blockquote><strong>¿Cómo se comportan los humanos y cómo se adaptan los interfaces cuando se produce la interacción de forma grupal?</strong></blockquote>\n<blockquote>\n<em>Por destacar alguna, una de las preguntas que nos hacemos en el laboratorio a diario es: </em><strong>¿Está la innovación desde la perspectiva del User Centric, alimentada al mismo ritmo que el avance de la tecnología?</strong>\n</blockquote>\n<p>A raíz de todas estas cuestiones y de los retos que suponen estas nuevas formas de interacción<strong> hemos querido aunar esfuerzos y plantearnos toda esta problemática como una línea de investigación global y única que enmarca multitud de retos e hipótesis emocionantes a validar.</strong></p>\n<p>Y para abordar este estudio, en el <strong>Innovation Technology Lab</strong> hemos hecho como haría todo buen investigador: hemos desgranado el problema, definiéndolo en <strong>“modelos de interacción” a modo de principios y estableciendo una hoja de ruta para guiar nuestra investigación en HCI</strong>.</p>\n<h3>Las áreas de estudio de las que partimos son estas:</h3>\n<h4>1. La Interacción natural cobra por fín su acepción original</h4>\n<p>Nos ha costado, pero en este sentido la problemática de <strong>la tendencia hacia la zero UI </strong>(la eliminación del interfaces puros o gráficos) con la entrada en escena de <strong>la voz y la conversación como interfaz para los asistentes de Google, Apple o Amazon es uno de los puntos de más actualidad y que hemos trabajado más en el Innovation Technology Lab.</strong></p>\n<p>El inminente aumento de la interacción gestual con la irrupción de la realidad mixta y la interacción en los espacios físicos también suponen un reto para futuros patrones de interacción, ya que es un área que está casi completamente inexplorada. <strong>En el Lab estamos desarrollando nuevas maneras de interacción basadas en gestos y probando cuáles son las posibilidades que ofrecen.</strong></p>\n<p>Nos parece interesante también las implicaciones que tiene que los interfaces interpreten nuestras emociones para adaptarse a nosotros con la ayuda de la inteligencia artificial, lo que ayudará a la evolución de la llamada Affecting Computing y los sistemas de recomendación aplicados a interfaces.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*9_IVY0fhXULlq_u0jSIUzg.gif\"><figcaption>Experimento para validar Hipótesis sobre el reto de interacción gestual para enriquecer flujos conversacionales realizado en el <strong>Innovation Technology Lab de </strong><a href=\"https://www.bbvanexttechnologies.com/\"><strong>BBVA Next Technologies</strong></a></figcaption></figure><h4>2. La interacción con interfaces gráficos da un paso más y sale definitivamente de las pantallas</h4>\n<p>Las spatial apps y las experiencias inmersivas nos trasladan a un mundo de experiencias totalmente diferente al que estábamos acostumbrados con apps tradicionales como las de Facebook o Instagram, donde la experiencia se limitaba a la pantalla de nuestros dispositivos.</p>\n<p>Como no podía ser de otro modo, las posibilidades aquí son muchas y las preguntas que nos surgen en este punto también ;)</p>\n<blockquote><strong>¿Cómo nos comportaremos ante interfaces holográficos que invadan nuestro entorno?</strong></blockquote>\n<blockquote><strong>¿Qué implicaciones tiene la interacción con pantallas de gran escala con voz o gestos en entornos abiertos y multiusuario?</strong></blockquote>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/636/1*sEbxiIh0k2Blt23cqQ4mQw.gif\"><figcaption>Experimento de interacción en espacios físicos utilizando nuestro entorno de pruebas controlado en VR, realizado en el <strong>Innovation Technology Lab de BBVA Next Technologies</strong></figcaption></figure><h4>3. Cómo afecta a la experiencia de los usuarios que, gracias a la tecnología, las plataformas y los entornos puedan mezclarse.</h4>\n<p>En los últimos 10 años nos hemos acostumbrado a <strong>experiencias enfocadas al consumo de usuarios únicos</strong> en un solo dispositivo-canal, y sobre todo (y más en la época reciente) a que estas experiencias estén <strong>monopolizadas por los móviles</strong>.</p>\n<p>No <strong>sabemos cómo deben comportarse los interfaces multiusuario ni mucho menos cómo los humanos trabajamos juntos ante un mismo interfaz.</strong></p>\n<p>Esto supone un reto a explorar porque sobre todo el futuro de los entornos de trabajo y el workflow de los trabajadores depende en gran medida de todo lo que consigamos</p>\n<h4>Multicanalidad</h4>\n<p>Con estas premisas, <strong>la multimodalidad, los entornos multiusuario y reactivos son las áreas de investigación donde estamos centrando nuestros esfuerzos.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lxJbWcGa9db_tsS0aHdlLQ.png\"><figcaption>Flujo de interacción para definir los comportamientos de un experimento de interacción en espacios físicos realizado en el <strong>Innovation Technology Lab de BBVA Next Technologies</strong></figcaption></figure><h4>Estamos afinando el tiro</h4>\n<p>Como hemos dicho, en el Lab hemos condensado estas problemas en <strong>línea de investigación global</strong> de la que<strong>, a medida que vayamos obteniendo conclusiones, extraeremos retos que iremos abordando en ciclos de diseño que intentamos explorar, siempre impulsados ​​por hipótesis.</strong></p>\n<p><strong>Y en ello estamos.</strong> Ya estamos trabajando en esta hoja de ruta y esperamos poder compartir con vosotros resultados más pronto que tarde.</p>\n<p>Mientras, seguimos ensayando, probando y jugando con todas las posibilidades que ofrece la interacción de humanos-usuarios y la tecnología, que actualmente se encuentra <strong>en su nueva (que no última cruzada): dejar a un lado lo virtual para conquistar el mundo físico.</strong></p>\n<h4>Info:</h4>\n<p>Si quieres estar al tanto de lo que hacemos, obtener <a href=\"https://beeva-labs.github.io/webtesting/dist/#/\">info y documentación</a> del <strong>Innovation Technology Lab de BBVA de Next Technologies, te esperamos </strong><a href=\"https://beeva-labs.github.io/webtesting/dist/#/\"><strong>aquí</strong></a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=29f1b21c6dbc\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/a-la-conquista-del-espacio-29f1b21c6dbc\">A la conquista del espacio</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*tHL1Mgy3x5JBkPUpIcilPQ.png\"></figure><p>Interacción en espacios físicos, el gran reto de la HCI…</p>\n<p>En su conquista del espacio y de retos impensables (impensables solo para una generación anterior, claro), la tecnología está llevando a nuestro día a día experiencias que hasta hace poco apenas podríamos imaginar. Una de las posibilidades que más nos emociona actualmente es la llegada de <strong>la experiencia digital al mundo físico</strong>, algo que supone multitud de implicaciones que sólo estamos empezando a explorar.</p>\n<blockquote>En esta recién estrenada frontera, la que supone la <strong>conquista de los espacios físicos</strong> por el mundo digital, pensamos que el reto será conseguir experiencias satisfactorias que aporten valor para el usuario.</blockquote>\n<p>Y las cosas avanzan rápido: ya se están ampliando cada vez más las posibilidades de interacción <strong>más allá de las pantallas y las interfaces gráficas “clásicas” y </strong>gracias a esta expansión <strong>se están creando sistemas nuevos y una forma diferente en la que las personas interactuamos con los dispositivos</strong>, algo que supone multitud de retos a los que nos enfrentamos quienes trabajamos en un laboratorio de HCI.</p>\n<h4><strong>Buscando las respuestas</strong></h4>\n<p>Y como no puede ser de otra manera, en esta tarea exploratoria en la que parece que todo es posible, a los investigadores HCI nos asaltan preguntas continuamente, del tipo:</p>\n<blockquote><strong>¿Cómo nos comportamos ante coches y otros dispositivos autónomos?</strong></blockquote>\n<blockquote><strong>¿Como interactuamos con los asistentes de voz?</strong></blockquote>\n<blockquote><strong>¿Está la tecnología deshumanizada?</strong></blockquote>\n<blockquote><strong>¿Cómo está afectando esta nueva frontera de la inteligencia artificial a nuestra privacidad?</strong></blockquote>\n<blockquote><strong>¿Cómo se comportan los humanos y cómo se adaptan los interfaces cuando se produce la interacción de forma grupal?</strong></blockquote>\n<blockquote>\n<em>Por destacar alguna, una de las preguntas que nos hacemos en el laboratorio a diario es: </em><strong>¿Está la innovación desde la perspectiva del User Centric, alimentada al mismo ritmo que el avance de la tecnología?</strong>\n</blockquote>\n<p>A raíz de todas estas cuestiones y de los retos que suponen estas nuevas formas de interacción<strong> hemos querido aunar esfuerzos y plantearnos toda esta problemática como una línea de investigación global y única que enmarca multitud de retos e hipótesis emocionantes a validar.</strong></p>\n<p>Y para abordar este estudio, en el <strong>Innovation Technology Lab</strong> hemos hecho como haría todo buen investigador: hemos desgranado el problema, definiéndolo en <strong>“modelos de interacción” a modo de principios y estableciendo una hoja de ruta para guiar nuestra investigación en HCI</strong>.</p>\n<h3>Las áreas de estudio de las que partimos son estas:</h3>\n<h4>1. La Interacción natural cobra por fín su acepción original</h4>\n<p>Nos ha costado, pero en este sentido la problemática de <strong>la tendencia hacia la zero UI </strong>(la eliminación del interfaces puros o gráficos) con la entrada en escena de <strong>la voz y la conversación como interfaz para los asistentes de Google, Apple o Amazon es uno de los puntos de más actualidad y que hemos trabajado más en el Innovation Technology Lab.</strong></p>\n<p>El inminente aumento de la interacción gestual con la irrupción de la realidad mixta y la interacción en los espacios físicos también suponen un reto para futuros patrones de interacción, ya que es un área que está casi completamente inexplorada. <strong>En el Lab estamos desarrollando nuevas maneras de interacción basadas en gestos y probando cuáles son las posibilidades que ofrecen.</strong></p>\n<p>Nos parece interesante también las implicaciones que tiene que los interfaces interpreten nuestras emociones para adaptarse a nosotros con la ayuda de la inteligencia artificial, lo que ayudará a la evolución de la llamada Affecting Computing y los sistemas de recomendación aplicados a interfaces.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*9_IVY0fhXULlq_u0jSIUzg.gif\"><figcaption>Experimento para validar Hipótesis sobre el reto de interacción gestual para enriquecer flujos conversacionales realizado en el <strong>Innovation Technology Lab de </strong><a href=\"https://www.bbvanexttechnologies.com/\"><strong>BBVA Next Technologies</strong></a></figcaption></figure><h4>2. La interacción con interfaces gráficos da un paso más y sale definitivamente de las pantallas</h4>\n<p>Las spatial apps y las experiencias inmersivas nos trasladan a un mundo de experiencias totalmente diferente al que estábamos acostumbrados con apps tradicionales como las de Facebook o Instagram, donde la experiencia se limitaba a la pantalla de nuestros dispositivos.</p>\n<p>Como no podía ser de otro modo, las posibilidades aquí son muchas y las preguntas que nos surgen en este punto también ;)</p>\n<blockquote><strong>¿Cómo nos comportaremos ante interfaces holográficos que invadan nuestro entorno?</strong></blockquote>\n<blockquote><strong>¿Qué implicaciones tiene la interacción con pantallas de gran escala con voz o gestos en entornos abiertos y multiusuario?</strong></blockquote>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/636/1*sEbxiIh0k2Blt23cqQ4mQw.gif\"><figcaption>Experimento de interacción en espacios físicos utilizando nuestro entorno de pruebas controlado en VR, realizado en el <strong>Innovation Technology Lab de BBVA Next Technologies</strong></figcaption></figure><h4>3. Cómo afecta a la experiencia de los usuarios que, gracias a la tecnología, las plataformas y los entornos puedan mezclarse.</h4>\n<p>En los últimos 10 años nos hemos acostumbrado a <strong>experiencias enfocadas al consumo de usuarios únicos</strong> en un solo dispositivo-canal, y sobre todo (y más en la época reciente) a que estas experiencias estén <strong>monopolizadas por los móviles</strong>.</p>\n<p>No <strong>sabemos cómo deben comportarse los interfaces multiusuario ni mucho menos cómo los humanos trabajamos juntos ante un mismo interfaz.</strong></p>\n<p>Esto supone un reto a explorar porque sobre todo el futuro de los entornos de trabajo y el workflow de los trabajadores depende en gran medida de todo lo que consigamos</p>\n<h4>Multicanalidad</h4>\n<p>Con estas premisas, <strong>la multimodalidad, los entornos multiusuario y reactivos son las áreas de investigación donde estamos centrando nuestros esfuerzos.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lxJbWcGa9db_tsS0aHdlLQ.png\"><figcaption>Flujo de interacción para definir los comportamientos de un experimento de interacción en espacios físicos realizado en el <strong>Innovation Technology Lab de BBVA Next Technologies</strong></figcaption></figure><h4>Estamos afinando el tiro</h4>\n<p>Como hemos dicho, en el Lab hemos condensado estas problemas en <strong>línea de investigación global</strong> de la que<strong>, a medida que vayamos obteniendo conclusiones, extraeremos retos que iremos abordando en ciclos de diseño que intentamos explorar, siempre impulsados ​​por hipótesis.</strong></p>\n<p><strong>Y en ello estamos.</strong> Ya estamos trabajando en esta hoja de ruta y esperamos poder compartir con vosotros resultados más pronto que tarde.</p>\n<p>Mientras, seguimos ensayando, probando y jugando con todas las posibilidades que ofrece la interacción de humanos-usuarios y la tecnología, que actualmente se encuentra <strong>en su nueva (que no última cruzada): dejar a un lado lo virtual para conquistar el mundo físico.</strong></p>\n<h4>Info:</h4>\n<p>Si quieres estar al tanto de lo que hacemos, obtener <a href=\"https://beeva-labs.github.io/webtesting/dist/#/\">info y documentación</a> del <strong>Innovation Technology Lab de BBVA de Next Technologies, te esperamos </strong><a href=\"https://beeva-labs.github.io/webtesting/dist/#/\"><strong>aquí</strong></a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=29f1b21c6dbc\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/a-la-conquista-del-espacio-29f1b21c6dbc\">A la conquista del espacio</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "report",
            "hci",
            "beeva-labs",
            "gesture-recognition",
            "vr"
        ],
        "area": "insigths",
        "active": true,
        "id": "_lqa41ag83"
    },
    {
        "title": "Enfrentándonos al reconocimiento del habla en un dispositivo conversacional propio",
        "pubDate": "2018-09-10 09:43:56",
        "link": "https://labs.beeva.com/enfrent%C3%A1ndonos-al-reconocimiento-del-habla-en-un-dispositivo-conversacional-propio-83308808d998?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/83308808d998",
        "author": "Luismi Fuentes",
        "thumbnail": "https://cdn-images-1.medium.com/max/730/1*REKpYyO8wqZq4RCXbPeEQA.jpeg",
        "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*REKpYyO8wqZq4RCXbPeEQA.jpeg\"></figure><p>Siguiendo con la investigación en interfaces conversacionales dentro del Lab, afrontamos la creación un SmartMirror, un dispositivo en el que se combina voz e imagen en la interacción con el usuario, por lo cual necesitábamos:</p>\n<ul>\n<li>Un servicio de reconocimiento de voz para que el espejo entendiera nuestras palabras, Automatic Speech Recognition (ASR)</li>\n<li>Otro servicio para que devolviese respuestas con su propia voz y poder interactuar llamado Text-To-Speech (TTS)</li>\n</ul>\n<p>Para estos “problemas” tenemos muchas opciones incluyendo soluciones de Amazon, Azure y Google. Las 3 tienen los servicios que necesitábamos, a la hora de probar y demás, para nuestro caso nos decantamos por las siguientes:</p>\n<p>Automatic-Speech-Recognition (ASR):</p>\n<p>Nos decidimos por Google, funciona muy bien el reconocimiento y proporciona bastantes herramientas y facilidades.</p>\n<p><strong>Google — Automatic-Speech-Recognition</strong>:</p>\n<p>Este servicio nos permite mediante vía streaming, capturar la voz del usuario y convertirla a texto para su posterior procesamiento.</p>\n<p>La forma de uso es mediante llamadas a la API de Google, se crea un streaming con el parámetro “single utterance” activado, reconoce las frases que le digamos y termina el reconocimiento devolviendo el texto resultante de la interacción.</p>\n<p>Teníamos la necesidad de poder reconocer mediante habla dnis y emails, a veces no reconocía estas situaciones como desearíamos, por lo que se tuvieron que crear algunas funciones para resolver esas situaciones.</p>\n<p>En este caso se han usado las librerías que hay para Python por parte de Google, la cual pueden facilitar mucho el uso. Para interactuar con el micrófono que hace uso de la librería pyAudio de Python.</p>\n<p>Text-To-Speech (TTS):</p>\n<p>En este caso empezamos con Azure, que es la que más nos gustó tanto por la voz como pronunciación en textos largos.</p>\n<p><strong>Azure — Bing Text-To-Speech</strong>:</p>\n<p>Este servicio nos permite pasarle una cadena de texto y nos genera un archivo de audio, el cual podemos configurar mediante SSML, pasándoselo en el ‘HEADER’ de la llamada a la API.</p>\n<p>En el caso de nuestro idioma tenemos disponibles 3 opciones:</p>\n<ul>\n<li>HelenaRUS</li>\n<li>Laura Apollpo</li>\n<li>Pablo Apollo</li>\n</ul>\n<p>La que más nos gustó es HelenaRUS, es muy parecida a Cortana y pronuncia bastante bien. Este servicio te devuelve un archivo (.WAV), para la reproducción del mismo se ha utilizado la librería pyAudio</p>\n<p>La segunda alternativa que probamos es Polly de Amazon, con la que al final nos quedamos, nos convenció más la voz.</p>\n<p><strong>Amazon AWS Polly Text-To-Speech</strong>:</p>\n<p>Este servicio al igual que Bing, nos permite transformar texto en habla. Para este servicio la voz que más nos gustó fue Enrique. Por lo demás, se usan muy parecido, mediante llamadas a la API, las diferencias están en los parámetros de la propia llamada y que esta API nos devuelve un archivo .mp3, para la reproducción del audio generado se ha utilizado la librería playsound.</p>\n<p>El motivo de utilizar dos librerías para reproducción de audio es que cada una asegura la reproducción de ese formato de archivo en concreto, así nos aseguramos de que se utiliza una librería específica para cada formato y no tener problemas.</p>\n<p>En algunos casos hemos utilizado las API REST y en otros el SDK, para ello necesitamos los enlaces correctos para nuestra ubicación y las API_KEYS que se consiguen mediante pago (disponemos de un periodo gratis al registrarnos).</p>\n<p>Como lenguaje se escogió Python por su fácil entendimiento. Para las llamadas a las APIs, la librería Requests.</p>\n<p>Se han escogido estas tecnologías porque ya las conocíamos, funcionan bastante bien y permiten varias posibilidades de configuración.</p>\n<p><strong>Conclusión</strong></p>\n<p>Si queremos hacer el espejo interactivo mediante conversación, estas tecnologías permiten hacerlo y bastante bien, además, nos permite añadir tantas opciones como queramos ya que todo se controla mediante palabras clave.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=83308808d998\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/enfrent%C3%A1ndonos-al-reconocimiento-del-habla-en-un-dispositivo-conversacional-propio-83308808d998\">Enfrentándonos al reconocimiento del habla en un dispositivo conversacional propio</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*REKpYyO8wqZq4RCXbPeEQA.jpeg\"></figure><p>Siguiendo con la investigación en interfaces conversacionales dentro del Lab, afrontamos la creación un SmartMirror, un dispositivo en el que se combina voz e imagen en la interacción con el usuario, por lo cual necesitábamos:</p>\n<ul>\n<li>Un servicio de reconocimiento de voz para que el espejo entendiera nuestras palabras, Automatic Speech Recognition (ASR)</li>\n<li>Otro servicio para que devolviese respuestas con su propia voz y poder interactuar llamado Text-To-Speech (TTS)</li>\n</ul>\n<p>Para estos “problemas” tenemos muchas opciones incluyendo soluciones de Amazon, Azure y Google. Las 3 tienen los servicios que necesitábamos, a la hora de probar y demás, para nuestro caso nos decantamos por las siguientes:</p>\n<p>Automatic-Speech-Recognition (ASR):</p>\n<p>Nos decidimos por Google, funciona muy bien el reconocimiento y proporciona bastantes herramientas y facilidades.</p>\n<p><strong>Google — Automatic-Speech-Recognition</strong>:</p>\n<p>Este servicio nos permite mediante vía streaming, capturar la voz del usuario y convertirla a texto para su posterior procesamiento.</p>\n<p>La forma de uso es mediante llamadas a la API de Google, se crea un streaming con el parámetro “single utterance” activado, reconoce las frases que le digamos y termina el reconocimiento devolviendo el texto resultante de la interacción.</p>\n<p>Teníamos la necesidad de poder reconocer mediante habla dnis y emails, a veces no reconocía estas situaciones como desearíamos, por lo que se tuvieron que crear algunas funciones para resolver esas situaciones.</p>\n<p>En este caso se han usado las librerías que hay para Python por parte de Google, la cual pueden facilitar mucho el uso. Para interactuar con el micrófono que hace uso de la librería pyAudio de Python.</p>\n<p>Text-To-Speech (TTS):</p>\n<p>En este caso empezamos con Azure, que es la que más nos gustó tanto por la voz como pronunciación en textos largos.</p>\n<p><strong>Azure — Bing Text-To-Speech</strong>:</p>\n<p>Este servicio nos permite pasarle una cadena de texto y nos genera un archivo de audio, el cual podemos configurar mediante SSML, pasándoselo en el ‘HEADER’ de la llamada a la API.</p>\n<p>En el caso de nuestro idioma tenemos disponibles 3 opciones:</p>\n<ul>\n<li>HelenaRUS</li>\n<li>Laura Apollpo</li>\n<li>Pablo Apollo</li>\n</ul>\n<p>La que más nos gustó es HelenaRUS, es muy parecida a Cortana y pronuncia bastante bien. Este servicio te devuelve un archivo (.WAV), para la reproducción del mismo se ha utilizado la librería pyAudio</p>\n<p>La segunda alternativa que probamos es Polly de Amazon, con la que al final nos quedamos, nos convenció más la voz.</p>\n<p><strong>Amazon AWS Polly Text-To-Speech</strong>:</p>\n<p>Este servicio al igual que Bing, nos permite transformar texto en habla. Para este servicio la voz que más nos gustó fue Enrique. Por lo demás, se usan muy parecido, mediante llamadas a la API, las diferencias están en los parámetros de la propia llamada y que esta API nos devuelve un archivo .mp3, para la reproducción del audio generado se ha utilizado la librería playsound.</p>\n<p>El motivo de utilizar dos librerías para reproducción de audio es que cada una asegura la reproducción de ese formato de archivo en concreto, así nos aseguramos de que se utiliza una librería específica para cada formato y no tener problemas.</p>\n<p>En algunos casos hemos utilizado las API REST y en otros el SDK, para ello necesitamos los enlaces correctos para nuestra ubicación y las API_KEYS que se consiguen mediante pago (disponemos de un periodo gratis al registrarnos).</p>\n<p>Como lenguaje se escogió Python por su fácil entendimiento. Para las llamadas a las APIs, la librería Requests.</p>\n<p>Se han escogido estas tecnologías porque ya las conocíamos, funcionan bastante bien y permiten varias posibilidades de configuración.</p>\n<p><strong>Conclusión</strong></p>\n<p>Si queremos hacer el espejo interactivo mediante conversación, estas tecnologías permiten hacerlo y bastante bien, además, nos permite añadir tantas opciones como queramos ya que todo se controla mediante palabras clave.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=83308808d998\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/enfrent%C3%A1ndonos-al-reconocimiento-del-habla-en-un-dispositivo-conversacional-propio-83308808d998\">Enfrentándonos al reconocimiento del habla en un dispositivo conversacional propio</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "machine-intelligence",
            "beeva-labs",
            "nlp",
            "knowledge"
        ],
        "area": "insigths",
        "active": true,
        "id": "_jzhnqnkmn"
    },
    {
        "title": "La visión artificial para un SmartMirror",
        "pubDate": "2018-08-28 07:49:13",
        "link": "https://labs.beeva.com/la-visi%C3%B3n-artificial-para-un-smartmirror-82b2589814e1?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/82b2589814e1",
        "author": "ALBERTO RICI VAZQUEZ",
        "thumbnail": "https://cdn-images-1.medium.com/max/730/1*y9DFoeb54pIP_beMrbIWoQ.jpeg",
        "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*y9DFoeb54pIP_beMrbIWoQ.jpeg\"></figure><p>En el departamento de Innovación de <a href=\"https://www.beeva.com/\">BEEVA</a>, nos encontramos con un proyecto en el que necesitábamos construir y desarrollar un espejo inteligente, un <strong>SmartMirror</strong>. Un espejo con una pantalla adjunta que muestra una página web estática con la que se pudiera interactuar. Un SmartMirror es una combinación de muchas de las cosas con las que trabajamos: diseño de interfaz de usuario, visión artificial, tecnologías web, etc.</p>\n<p>El motivo por el que se decidió utilizar la visión artificial es que el SmartMirror cuenta con una cámara entre sus distintos tipos de sensores. Por ello, se pensó en la visión artificial, a la cual se le encontraron dos casos principales.</p>\n<h4>Caso de uso</h4>\n<p>El primer caso que nos planteamos, es cuando debe activarse el espejo. Entonces se pensó en que cuando la cámara detectase que hay una cara enfrente del espejo entonces es cuando se tendría que activar. El siguiente caso se podría considerar como una medida de <strong>seguridad</strong> y también de dar más <strong>personalidad</strong> al espejo cuando hable con el usuario, nos referimos a la identificación del usuario.</p>\n<p>Esto puede ser como medida de seguridad para poder acceder a las funcionalidades del espejo, es decir, si no te reconoce no se podría utilizar el espejo. Además, añade una capa de <strong>personalización</strong> para el usuario, permite asociar datos personales a un usuario y mejorar su experiencia.</p>\n<p>En nuestro proyecto concretamente se ha utilizado para acceder a las funcionalidades y que el espejo hable al usuario por su nombre. Estas dos funcionalidades se han desarrollado en <strong>Python</strong> como lenguaje de programación y se decidió desarrollarlas como servicios que se consumen como <strong>APIs</strong>.</p>\n<h4><strong>Detección de caras</strong></h4>\n<p>Este servicio nos permite detectar que hay caras delante de una cámara y para su desarrollo se han utilizado las librerías de <strong>dLib</strong> y <strong>OpenCV</strong>. Estas se han utilizado por el simple hecho de que son <strong>Open Source</strong>, no necesitan ningún tipo de licencia y funcionan perfectamente para el caso.También porque con estas librerías se realiza muy bien la función de detectar caras. Además son muy <strong>eficientes</strong> en términos de cómputo.</p>\n<p>La detección de caras sigue los siguientes pasos:</p>\n<ol>\n<li>Sacamos una imagen mediante la cámara y la pasamos a escala de grises, esto mediante las herramientas de la librería de OpenCV.</li>\n<li>Una vez obtenida la imagen en escala de grises se pasa al formato requerido para que la librería dLib pueda detectar todas las caras de la imagen.</li>\n<li>Se realiza la detección de caras mediante el método integrado de dLib que utiliza la técnica de <strong>HOG</strong> (Histogramas de Gradientes Orientados).</li>\n<li>En nuestro caso, de todas las caras posibles nos quedamos con la cara más cercana.</li>\n</ol>\n<p>La detección de caras se realiza de manera cíclica cada 2 segundos, para comprobar que sigue habiendo una persona delante o en otro caso, cerrar la sesión después de cierto tiempo.</p>\n<h4><strong>Identificación de usuarios</strong></h4>\n<p>Este servicio es el que nos permite identificar a los usuarios, para ello se ha utilizado<strong> Face API</strong> de Microsoft <strong>Azure</strong>. Ofrece un servicio completo y estable, que se ajusta a nuestro caso de uso.</p>\n<p>Para identificar a una persona previamente se ha tenido que hacer una serie de pasos. Lo primero es crear en los servicios de Face API un grupo y a continuación seguir los siguientes pasos para añadir las personas que pertenecen a ese grupo.</p>\n<ol>\n<li>Se crea a la persona, indicando el grupo al que pertenece y el nombre u otro tipo de identificación de la misma.</li>\n<li>Una vez creada la persona, a esta se le añaden imágenes que la identifiquen.Para nuestro proyecto concretamente, por persona se han introducido tres fotos.</li>\n<li>Cuando hayamos terminado de crear a las personas y añadiles las imágenes, lo que hay que hacer es entrenar el grupo para que pueda identificar a las distintas personas.</li>\n</ol>\n<p>Una vez esto, el procedimiento para identificar a una persona sigue los siguientes pasos.</p>\n<ol>\n<li>El primer paso es hacerle una foto al usuario y enviarla al servicio de Azure para obtener el face id de la foto.</li>\n<li>Obtenido el face id, lo que hacemos es llamar a otro servicio, al cual se le indica el grupo del que se quiere comprobar si dicha persona pertenece o no y el face id.</li>\n</ol>\n<p>Como medida de optimización no se llama siempre que se detecte una cara a la funcionalidad de identificación, sino que cuando se detecta una cara se espera a que el usuario diga la wake up word que se establezca para activar el espejo y entonces se procede a identificar al usuario.</p>\n<h4>Conclusión</h4>\n<p>Como conclusión, si se desea que el SmartMirror no esté constantemente consumiendo una gran cantidad de recursos y tenga un alto grado de acierto a la hora de identificar a los usuarios y detectar caras, los servicios comentados anteriormente son los idóneos</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=82b2589814e1\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/la-visi%C3%B3n-artificial-para-un-smartmirror-82b2589814e1\">La visión artificial para un SmartMirror</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*y9DFoeb54pIP_beMrbIWoQ.jpeg\"></figure><p>En el departamento de Innovación de <a href=\"https://www.beeva.com/\">BEEVA</a>, nos encontramos con un proyecto en el que necesitábamos construir y desarrollar un espejo inteligente, un <strong>SmartMirror</strong>. Un espejo con una pantalla adjunta que muestra una página web estática con la que se pudiera interactuar. Un SmartMirror es una combinación de muchas de las cosas con las que trabajamos: diseño de interfaz de usuario, visión artificial, tecnologías web, etc.</p>\n<p>El motivo por el que se decidió utilizar la visión artificial es que el SmartMirror cuenta con una cámara entre sus distintos tipos de sensores. Por ello, se pensó en la visión artificial, a la cual se le encontraron dos casos principales.</p>\n<h4>Caso de uso</h4>\n<p>El primer caso que nos planteamos, es cuando debe activarse el espejo. Entonces se pensó en que cuando la cámara detectase que hay una cara enfrente del espejo entonces es cuando se tendría que activar. El siguiente caso se podría considerar como una medida de <strong>seguridad</strong> y también de dar más <strong>personalidad</strong> al espejo cuando hable con el usuario, nos referimos a la identificación del usuario.</p>\n<p>Esto puede ser como medida de seguridad para poder acceder a las funcionalidades del espejo, es decir, si no te reconoce no se podría utilizar el espejo. Además, añade una capa de <strong>personalización</strong> para el usuario, permite asociar datos personales a un usuario y mejorar su experiencia.</p>\n<p>En nuestro proyecto concretamente se ha utilizado para acceder a las funcionalidades y que el espejo hable al usuario por su nombre. Estas dos funcionalidades se han desarrollado en <strong>Python</strong> como lenguaje de programación y se decidió desarrollarlas como servicios que se consumen como <strong>APIs</strong>.</p>\n<h4><strong>Detección de caras</strong></h4>\n<p>Este servicio nos permite detectar que hay caras delante de una cámara y para su desarrollo se han utilizado las librerías de <strong>dLib</strong> y <strong>OpenCV</strong>. Estas se han utilizado por el simple hecho de que son <strong>Open Source</strong>, no necesitan ningún tipo de licencia y funcionan perfectamente para el caso.También porque con estas librerías se realiza muy bien la función de detectar caras. Además son muy <strong>eficientes</strong> en términos de cómputo.</p>\n<p>La detección de caras sigue los siguientes pasos:</p>\n<ol>\n<li>Sacamos una imagen mediante la cámara y la pasamos a escala de grises, esto mediante las herramientas de la librería de OpenCV.</li>\n<li>Una vez obtenida la imagen en escala de grises se pasa al formato requerido para que la librería dLib pueda detectar todas las caras de la imagen.</li>\n<li>Se realiza la detección de caras mediante el método integrado de dLib que utiliza la técnica de <strong>HOG</strong> (Histogramas de Gradientes Orientados).</li>\n<li>En nuestro caso, de todas las caras posibles nos quedamos con la cara más cercana.</li>\n</ol>\n<p>La detección de caras se realiza de manera cíclica cada 2 segundos, para comprobar que sigue habiendo una persona delante o en otro caso, cerrar la sesión después de cierto tiempo.</p>\n<h4><strong>Identificación de usuarios</strong></h4>\n<p>Este servicio es el que nos permite identificar a los usuarios, para ello se ha utilizado<strong> Face API</strong> de Microsoft <strong>Azure</strong>. Ofrece un servicio completo y estable, que se ajusta a nuestro caso de uso.</p>\n<p>Para identificar a una persona previamente se ha tenido que hacer una serie de pasos. Lo primero es crear en los servicios de Face API un grupo y a continuación seguir los siguientes pasos para añadir las personas que pertenecen a ese grupo.</p>\n<ol>\n<li>Se crea a la persona, indicando el grupo al que pertenece y el nombre u otro tipo de identificación de la misma.</li>\n<li>Una vez creada la persona, a esta se le añaden imágenes que la identifiquen.Para nuestro proyecto concretamente, por persona se han introducido tres fotos.</li>\n<li>Cuando hayamos terminado de crear a las personas y añadiles las imágenes, lo que hay que hacer es entrenar el grupo para que pueda identificar a las distintas personas.</li>\n</ol>\n<p>Una vez esto, el procedimiento para identificar a una persona sigue los siguientes pasos.</p>\n<ol>\n<li>El primer paso es hacerle una foto al usuario y enviarla al servicio de Azure para obtener el face id de la foto.</li>\n<li>Obtenido el face id, lo que hacemos es llamar a otro servicio, al cual se le indica el grupo del que se quiere comprobar si dicha persona pertenece o no y el face id.</li>\n</ol>\n<p>Como medida de optimización no se llama siempre que se detecte una cara a la funcionalidad de identificación, sino que cuando se detecta una cara se espera a que el usuario diga la wake up word que se establezca para activar el espejo y entonces se procede a identificar al usuario.</p>\n<h4>Conclusión</h4>\n<p>Como conclusión, si se desea que el SmartMirror no esté constantemente consumiendo una gran cantidad de recursos y tenga un alto grado de acierto a la hora de identificar a los usuarios y detectar caras, los servicios comentados anteriormente son los idóneos</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=82b2589814e1\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/la-visi%C3%B3n-artificial-para-un-smartmirror-82b2589814e1\">La visión artificial para un SmartMirror</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "machine-intelligence",
            "knowledge",
            "vision-artificial",
            "microsoft-azure",
            "beeva-labs"
        ],
        "area": "insigths",
        "active": true,
        "id": "_m1bnbvmlj"
    },
    {
        "title": "ARKit: los centímetros importan",
        "pubDate": "2018-05-08 11:16:05",
        "link": "https://labs.beeva.com/arkit-los-cent%C3%ADmetros-importan-602db61d1c8d?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/602db61d1c8d",
        "author": "Sergio Santamaria",
        "thumbnail": "https://cdn-images-1.medium.com/max/700/1*B-OEM-eZ6g2kHXIiqS8LRQ.png",
        "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*B-OEM-eZ6g2kHXIiqS8LRQ.png\"></figure><p>Yo creo que casi todos los que seguimos las nuevas tecnologías de interfaz ,como la realidad aumentada, nos imaginamos un futuro en el que llevamos unas gafas, a través de las cuales podemos ver todo tipo de elementos virtuales integrados con la realidad y que nos facilitarán el día a día con todo tipo de información hiper contextualizada.</p>\n<a href=\"https://medium.com/media/7b81950dcaec2503cc984d2748bd6b32/href\">https://medium.com/media/7b81950dcaec2503cc984d2748bd6b32/href</a><p>Este video, aunque llevado al paroxismo de la saturación informativa, refleja en mucha medida lo que a todos nos gustaría que fuera al realidad aumentada y, en mi opinión, lo que tendrá que ser si algún día se quiere convertir en una tecnología de uso diario. Lo que hoy en día se denomina una <em>commodity</em>.</p>\n<p>Pero para que esto pueda ser una realidad se tienen que resolver dos barreras, muy relacionadas ambas con la precisión.</p>\n<p>La primera de ellas es el posicionamiento absoluto. Que cada elemento virtual aparezca allí donde nosotros deseemos y cuando deseemos, sin margen de error, es una pieza clave del desarrollo de la AR. Y es que no es lo mismo que un indicador virtual en un supermercado este en su preciso lugar, que 15 centímetros desplazado, señalando aquello que no estábamos buscando.</p>\n<p>A día de hoy existen <strong><em>3 maneras de posicionar un elemento virtual</em></strong> en un entorno real: con el uso de marcadores AR, mediante geolocalización o pidiendo la intervención del usuario que los ubique en el lugar que desee.</p>\n<p>La tercera la descartamos en todos esos casos en los que somos nosotros los que queremos decidir donde y cuando mostrarle al usuario el contenido.</p>\n<p>En el caso de los <strong><em>marcadores AR</em></strong>, es una solución que tiene sus limitaciones, y es que la detección de un marcador de manera adecuada depende de muchos factores que no controlamos ( que haya luz, que el marcador sea visible, que el marcador no se deteriore con el tiempo, etc ).</p>\n<p>Y por último con <strong><em>geolocalización</em></strong>, el margen de error en el posicionamiento, que además depende mucho de condiciones atmosféricas y del entorno, también la convierten en una solución inadmisible si necesitamos ubicar con máxima precisión un elemento en el entorno para que no quede descontextualizado y, por supuesto, tampoco nos soluciona nada en posicionamiento en interiores.</p>\n<p>El segundo gran reto al que se enfrenta la AR es la propia <strong><em>integración de los elementos AR</em></strong>, ¿a quien no le resulta del todo antinatural que un objeto virtual se vea a través de objetos y muros?. Y es que un elemento no tiene solo que poder ubicarse con precisión en el entorno, si no que tiene que integrarse totalmente con el.</p>\n<p>Para enfrentarse a este reto, la tecnología pasa necesariamente por un <strong><em>reconocimiento real y preciso del entorno</em></strong>, que se puede hacer de dos maneras. Bien mediante un sistema de <strong><em>rayos de baja frecuencia</em></strong> que haga un mapeo de este entorno, o bien mediante un sistema de <strong><em>visión artificial</em></strong> que reconozca todo aquello que la cámara ve en relación a su distancia, forma y tamaño.</p>\n<p>Ambas soluciones son buenas, en el primer caso, han mostrado buenos resultados en dispositivos como las <strong><em>hololens de Microsoft</em></strong>. Mientras que la segunda, opción por la que se ha decantado Apple para su <strong><em>ARkit</em></strong>, ofrece buenos resultados siempre que el dispositivo tenga una capacidad de calculo y analisis de imagenes en tiempo real muy por encima de los que ofrecen los dispositivos móviles a día de hoy.</p>\n<p>Pues bien, Arkit, y aquí es donde quería llegar, ha dado un pasito adelante en ambos sentidos con la actualización 1.5 de su tecnología de AR, aunque un pasito muy pequeño.</p>\n<p>Dos de las nuevas funcionalidades más importantes de su ARKit, en las que más hincapié hacen, son precisamente la detección de marcadores AR y el análisis del entorno.</p>\n<p>En <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> desarrollamos una app de testing para probar todas estas funcionalidades y poder sacar conclusiones, teniendo en cuenta siempre que ambas tienen visos de ser solo unas aproximaciones tempranas, y es que ambas funcionalidades adolecen de lo mismo, falta de precisión.</p>\n<a href=\"https://medium.com/media/1b816e75ee04483a3fe85473dd59e61d/href\">https://medium.com/media/1b816e75ee04483a3fe85473dd59e61d/href</a><p>Por un lado, aunque es un esfuerzo loable que hayan incluido la detección de marcadores (era una funcionalidad que se echaba en falta en muchos casos de uso ), está muy lejos de funcionar como otras soluciones hace ya tiempo implantadas ( véase <em>Vuforia</em>). Tarda más de lo acostumbrado en detectar el marcador y cuando lo hace tiene serios problemas para calcular la distancia y posición del mismo, por lo que en muchos casos os encontrareis con que genera el modelo virtual descolocado o torcido.</p>\n<p>Por otro lado, El reconocimiento del entorno se sirve de tecnologías de visión artificial que, por las carencias computacionales que mencionamos anteriormente, está lejos de ofrecer una precisión que nos resulte útil en algún modo para dotar a los elementos virtuales de oclusión con el entorno y respuesta física a el, como se puede observar en este video demo.</p>\n<a href=\"https://medium.com/media/57d0337736e398b26ee0bfb0309a3673/href\">https://medium.com/media/57d0337736e398b26ee0bfb0309a3673/href</a><p>Como conclusión, cabe destacar que es una excelente noticia que las principales compañías que apuestan por la realidad aumentada sepan perfectamente cuales son los <strong><em>retos a superar</em></strong>, y que las soluciones van por buen camino, aunque necesiten más tiempo y desarrollo.</p>\n<p>Ya cuando eso esté solucionado, pensaremos en esas gafas para AR comodas y versatiles con las que todos soñamos.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=602db61d1c8d\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/arkit-los-cent%C3%ADmetros-importan-602db61d1c8d\">ARKit: los centímetros importan</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*B-OEM-eZ6g2kHXIiqS8LRQ.png\"></figure><p>Yo creo que casi todos los que seguimos las nuevas tecnologías de interfaz ,como la realidad aumentada, nos imaginamos un futuro en el que llevamos unas gafas, a través de las cuales podemos ver todo tipo de elementos virtuales integrados con la realidad y que nos facilitarán el día a día con todo tipo de información hiper contextualizada.</p>\n<a href=\"https://medium.com/media/7b81950dcaec2503cc984d2748bd6b32/href\">https://medium.com/media/7b81950dcaec2503cc984d2748bd6b32/href</a><p>Este video, aunque llevado al paroxismo de la saturación informativa, refleja en mucha medida lo que a todos nos gustaría que fuera al realidad aumentada y, en mi opinión, lo que tendrá que ser si algún día se quiere convertir en una tecnología de uso diario. Lo que hoy en día se denomina una <em>commodity</em>.</p>\n<p>Pero para que esto pueda ser una realidad se tienen que resolver dos barreras, muy relacionadas ambas con la precisión.</p>\n<p>La primera de ellas es el posicionamiento absoluto. Que cada elemento virtual aparezca allí donde nosotros deseemos y cuando deseemos, sin margen de error, es una pieza clave del desarrollo de la AR. Y es que no es lo mismo que un indicador virtual en un supermercado este en su preciso lugar, que 15 centímetros desplazado, señalando aquello que no estábamos buscando.</p>\n<p>A día de hoy existen <strong><em>3 maneras de posicionar un elemento virtual</em></strong> en un entorno real: con el uso de marcadores AR, mediante geolocalización o pidiendo la intervención del usuario que los ubique en el lugar que desee.</p>\n<p>La tercera la descartamos en todos esos casos en los que somos nosotros los que queremos decidir donde y cuando mostrarle al usuario el contenido.</p>\n<p>En el caso de los <strong><em>marcadores AR</em></strong>, es una solución que tiene sus limitaciones, y es que la detección de un marcador de manera adecuada depende de muchos factores que no controlamos ( que haya luz, que el marcador sea visible, que el marcador no se deteriore con el tiempo, etc ).</p>\n<p>Y por último con <strong><em>geolocalización</em></strong>, el margen de error en el posicionamiento, que además depende mucho de condiciones atmosféricas y del entorno, también la convierten en una solución inadmisible si necesitamos ubicar con máxima precisión un elemento en el entorno para que no quede descontextualizado y, por supuesto, tampoco nos soluciona nada en posicionamiento en interiores.</p>\n<p>El segundo gran reto al que se enfrenta la AR es la propia <strong><em>integración de los elementos AR</em></strong>, ¿a quien no le resulta del todo antinatural que un objeto virtual se vea a través de objetos y muros?. Y es que un elemento no tiene solo que poder ubicarse con precisión en el entorno, si no que tiene que integrarse totalmente con el.</p>\n<p>Para enfrentarse a este reto, la tecnología pasa necesariamente por un <strong><em>reconocimiento real y preciso del entorno</em></strong>, que se puede hacer de dos maneras. Bien mediante un sistema de <strong><em>rayos de baja frecuencia</em></strong> que haga un mapeo de este entorno, o bien mediante un sistema de <strong><em>visión artificial</em></strong> que reconozca todo aquello que la cámara ve en relación a su distancia, forma y tamaño.</p>\n<p>Ambas soluciones son buenas, en el primer caso, han mostrado buenos resultados en dispositivos como las <strong><em>hololens de Microsoft</em></strong>. Mientras que la segunda, opción por la que se ha decantado Apple para su <strong><em>ARkit</em></strong>, ofrece buenos resultados siempre que el dispositivo tenga una capacidad de calculo y analisis de imagenes en tiempo real muy por encima de los que ofrecen los dispositivos móviles a día de hoy.</p>\n<p>Pues bien, Arkit, y aquí es donde quería llegar, ha dado un pasito adelante en ambos sentidos con la actualización 1.5 de su tecnología de AR, aunque un pasito muy pequeño.</p>\n<p>Dos de las nuevas funcionalidades más importantes de su ARKit, en las que más hincapié hacen, son precisamente la detección de marcadores AR y el análisis del entorno.</p>\n<p>En <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> desarrollamos una app de testing para probar todas estas funcionalidades y poder sacar conclusiones, teniendo en cuenta siempre que ambas tienen visos de ser solo unas aproximaciones tempranas, y es que ambas funcionalidades adolecen de lo mismo, falta de precisión.</p>\n<a href=\"https://medium.com/media/1b816e75ee04483a3fe85473dd59e61d/href\">https://medium.com/media/1b816e75ee04483a3fe85473dd59e61d/href</a><p>Por un lado, aunque es un esfuerzo loable que hayan incluido la detección de marcadores (era una funcionalidad que se echaba en falta en muchos casos de uso ), está muy lejos de funcionar como otras soluciones hace ya tiempo implantadas ( véase <em>Vuforia</em>). Tarda más de lo acostumbrado en detectar el marcador y cuando lo hace tiene serios problemas para calcular la distancia y posición del mismo, por lo que en muchos casos os encontrareis con que genera el modelo virtual descolocado o torcido.</p>\n<p>Por otro lado, El reconocimiento del entorno se sirve de tecnologías de visión artificial que, por las carencias computacionales que mencionamos anteriormente, está lejos de ofrecer una precisión que nos resulte útil en algún modo para dotar a los elementos virtuales de oclusión con el entorno y respuesta física a el, como se puede observar en este video demo.</p>\n<a href=\"https://medium.com/media/57d0337736e398b26ee0bfb0309a3673/href\">https://medium.com/media/57d0337736e398b26ee0bfb0309a3673/href</a><p>Como conclusión, cabe destacar que es una excelente noticia que las principales compañías que apuestan por la realidad aumentada sepan perfectamente cuales son los <strong><em>retos a superar</em></strong>, y que las soluciones van por buen camino, aunque necesiten más tiempo y desarrollo.</p>\n<p>Ya cuando eso esté solucionado, pensaremos en esas gafas para AR comodas y versatiles con las que todos soñamos.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=602db61d1c8d\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/arkit-los-cent%C3%ADmetros-importan-602db61d1c8d\">ARKit: los centímetros importan</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "arkit",
            "hci",
            "report",
            "beeva-labs",
            "augmented-reality"
        ],
        "area": "insigths",
        "active": true,
        "id": "_oty88n2mq"
    },
    {
        "title": "Blockchain en 5 minutos. Parte 3: Criptomonedas",
        "pubDate": "2018-03-20 16:25:47",
        "link": "https://labs.beeva.com/blockchain-en-5-minutos-parte-3-criptomonedas-e2f8a05910f9?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/e2f8a05910f9",
        "author": "Enrique Otero BEEVA",
        "thumbnail": "https://cdn-images-1.medium.com/max/730/1*GnaPEJkDLwxBheE_243utA.png",
        "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*GnaPEJkDLwxBheE_243utA.png\"></figure><p>El término <strong>Bitcoin</strong> y el concepto de <strong>criptomonedas</strong> ya forman parte de nuestro día a día. Han saltado de <em>reddit</em> a los informativos de <em>Antena3.</em> Y de <em>hackernews</em> a las páginas salmón de los principales periódicos de tirada nacional.</p>\n<p>Pero tal vez haya conceptos que se escapen al neófito. ¿Qué relación existe entre las diferentes criptomonedas? ¿y entre ellas y el concepto de <strong><em>blockchains</em></strong>?¿Quiénes son los mineros? ¿Cómo podría convertirme en uno de ellos? ¿Y debería vender mis acciones de empresas cotizadas del IBEX-35 e invertirlo todo en Litecoin o ZenCash?</p>\n<p>En este post intentaré dar respuesta en 5 minutos a algunas de estas preguntas. Continuando la serie iniciada por mi compañero <a href=\"https://medium.com/@samuelmh_beeva/\">Samuel Muñoz</a> con el post <a href=\"https://labs.beeva.com/blockchain-en-5-minutos-parte-1-introducci%C3%B3n-8139e467b9ba\"><strong>Blockchain en 5 minutos. Parte 1: Introducción</strong></a></p>\n<p>El primer ejemplo de criptomoneda ha sido el <strong>Bitcoin</strong>. Bitcoin puede ser entendido como un libro de cuentas o <em>ledger</em>, <strong>descentralizado</strong> entre los miembros de una red. Y en lugar de una autoridad central que respalde esta divisa, son los propios nodos de la red los que lo hacen.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/0*G3ffPDEZ0a-ixlA9.jpg\"><figcaption><a href=\"http://maxpixel.freegreatpicture.com/Bitcoin-Advantages-Benefits-Pros-Drawbacks-Cons-2574759\">http://maxpixel.freegreatpicture.com/Bitcoin-Advantages-Benefits-Pros-Drawbacks-Cons-2574759</a></figcaption></figure><p>Pero Bitcoin no es la única criptomoneda que existe. En webs como coinwarz podemos encontrar cerca de 200 criptomonedas, con diferentes características y relaciones entre sí. Como <em>Bitcoin cash</em>, <em>Ether</em>, <em>Litecoin</em>, <em>ZenCash</em>, <em>Monero</em>… y otras muchas. Y además, las criptomonedas tampoco son el único uso posible de la tecnología de cadenas de bloques, o<em> blockchains</em>.</p>\n<p>Más allá de servir de soporte a criptomonedas y libros de cuentas distribuidos, el carácter <strong>descentralizado</strong> e <strong>inmutable</strong> de la tecnología <em>blockchain</em> podría dar un giro a todos aquellos modelos de negocio basados en la existencia de autoridades centrales de confianza o intermediadores.</p>\n<h3>¿Cómo puedo participar? (I) Invirtiendo/especulando</h3>\n<p>El Bitcoin y el resto de criptomonedas han sido activos volátiles hasta la fecha, y especialmente en el último año. La cotización del Bitcoin ha tenido un crecimiento espectacular en 2017. Llegando a multiplicar su precio por 20 en apenas un año. Por este motivo podría considerarse como una de las mayores burbujas de la historia. Y es habitual cuando se habla de <strong>inversión, especulación y burbujas</strong>, comparar con el caso histórico de la <a href=\"http://www.eleconomista.es/divisas/noticias/8805544/12/17/El-bitcoin-supera-ya-a-los-tulipanes-como-la-burbuja-mas-grande-de-la-historia.html\">burbuja de los tulipanes (1634–1637)</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/495/0*p6JdJRZ3_gpPHX81.png\"><figcaption><a href=\"http://maxpixel.freegreatpicture.com/Eth-Ethereum-Block-Chain-Black-Bitcoin-2458552\">http://maxpixel.freegreatpicture.com/Eth-Ethereum-Block-Chain-Black-Bitcoin-2458552</a></figcaption></figure><p>Sea una burbuja o no, en cualquier operación de compraventa resulta recomendable intentar diferenciar el valor de un activo de su precio actual. Y en el caso de las criptomonedas resulta muy difícil hacer una estimación de su valor como medios de pago o valor refugio. Y también resulta muy complejo estimar el valor de la tecnología subyacente. ¿Merece la pena pagar por unidad de cómputo en la máquina virtual de Ethereum <a href=\"https://hackernoon.com/ether-purchase-power-df40a38c5a2f\">millones de veces más</a> que por una instancia en Amazon Web Services? Dependerá de los casos de uso. Además, el propio valor de la tecnología también estará relacionado con su grado de adopción ¿Podemos estimar el valor intrínseco de la red Ethereum a día de hoy? El grado de incertidumbre de cualquier estimación sería muy alto.</p>\n<p>Si decidimos comprar criptomonedas, podemos hacerlo de forma sencilla en casas de intercambio como <strong>Coinbase</strong>. Teniendo en cuenta las comisiones por compra y venta. O usos de tarjeta como medio de pago, por ejemplo.</p>\n<h3>¿Cómo puedo participar? (II) Minando</h3>\n<p>También podemos conseguir monedas a través del proceso denominado “minado”. Denominación utilizada por analogía con otros activos como el oro.</p>\n<p>La operación de minado corresponde con la resolución de una “<a href=\"https://elbitcoin.org/el-concepto-de-prueba-de-trabajo/\">prueba de trabajo</a>”. Estas pruebas de trabajo se basan generalmente en la resolución de un problema matemático de complejidad variable. En la práctica serían equivalentes a la resolución de un puzzle. Y el procedimiento de resolución es la fuerza bruta. Por este motivo, es importante conocer una estimación de nuestra capacidad de minado.</p>\n<p>Además, en la práctica debemos analizar los gastos de capital de la compra de hardware específico, si fuera el caso. Los gastos operativos, generalmente en consumo de corriente, o de infraestructuras en la nube. Y el precio actual de la criptomoneda que vayamos a minar.</p>\n<p>Por otro lado, la relación entre la capacidad de minado de nuestra infraestructura y la del resto de la red determinará el tiempo medio de minado de nuestro primer bloque. Este tiempo es aleatorio, pero debemos tener en cuenta su valor medio. Porque nuestro retorno de inversión puede no llegar en forma de pequeños incrementos diarios, sino de una gran cantidad conseguida tras un largo periodo sin conseguir nada. Por ese motivo puede resultar más recomendable unirse a un grupo más grande de mineros, para diversificar riesgo y beneficio. Estos grupos pueden ser muy grandes, y se conocen como un <em>pool</em> de minado. En este caso riesgo y beneficio están más repartidos, y son más previsibles. A cambio se deben tener en cuenta las comisiones por pertenencia a este <em>pool</em>.</p>\n<p>Existen webs que facilitan la estimación de beneficios en base a todas estas variables, como <a href=\"https://www.cryptocompare.com/mining/calculator/eth?HashingPower=20&amp;HashingUnit=MH%2Fs&amp;PowerConsumption=140&amp;CostPerkWh=0.12&amp;MiningPoolFee=0\">cryptocompare.com</a> o <a href=\"https://www.coinwarz.com/calculators/\">coinwarz.com</a></p>\n<p><em>Fuente imagen principal: </em><a href=\"https://pixabay.com/es/criptomoneda-negocio-las-finanzas-3085139/\"><em>https://pixabay.com/es/criptomoneda-negocio-las-finanzas-3085139/</em></a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e2f8a05910f9\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/blockchain-en-5-minutos-parte-3-criptomonedas-e2f8a05910f9\">Blockchain en 5 minutos. Parte 3: Criptomonedas</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*GnaPEJkDLwxBheE_243utA.png\"></figure><p>El término <strong>Bitcoin</strong> y el concepto de <strong>criptomonedas</strong> ya forman parte de nuestro día a día. Han saltado de <em>reddit</em> a los informativos de <em>Antena3.</em> Y de <em>hackernews</em> a las páginas salmón de los principales periódicos de tirada nacional.</p>\n<p>Pero tal vez haya conceptos que se escapen al neófito. ¿Qué relación existe entre las diferentes criptomonedas? ¿y entre ellas y el concepto de <strong><em>blockchains</em></strong>?¿Quiénes son los mineros? ¿Cómo podría convertirme en uno de ellos? ¿Y debería vender mis acciones de empresas cotizadas del IBEX-35 e invertirlo todo en Litecoin o ZenCash?</p>\n<p>En este post intentaré dar respuesta en 5 minutos a algunas de estas preguntas. Continuando la serie iniciada por mi compañero <a href=\"https://medium.com/@samuelmh_beeva/\">Samuel Muñoz</a> con el post <a href=\"https://labs.beeva.com/blockchain-en-5-minutos-parte-1-introducci%C3%B3n-8139e467b9ba\"><strong>Blockchain en 5 minutos. Parte 1: Introducción</strong></a></p>\n<p>El primer ejemplo de criptomoneda ha sido el <strong>Bitcoin</strong>. Bitcoin puede ser entendido como un libro de cuentas o <em>ledger</em>, <strong>descentralizado</strong> entre los miembros de una red. Y en lugar de una autoridad central que respalde esta divisa, son los propios nodos de la red los que lo hacen.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/0*G3ffPDEZ0a-ixlA9.jpg\"><figcaption><a href=\"http://maxpixel.freegreatpicture.com/Bitcoin-Advantages-Benefits-Pros-Drawbacks-Cons-2574759\">http://maxpixel.freegreatpicture.com/Bitcoin-Advantages-Benefits-Pros-Drawbacks-Cons-2574759</a></figcaption></figure><p>Pero Bitcoin no es la única criptomoneda que existe. En webs como coinwarz podemos encontrar cerca de 200 criptomonedas, con diferentes características y relaciones entre sí. Como <em>Bitcoin cash</em>, <em>Ether</em>, <em>Litecoin</em>, <em>ZenCash</em>, <em>Monero</em>… y otras muchas. Y además, las criptomonedas tampoco son el único uso posible de la tecnología de cadenas de bloques, o<em> blockchains</em>.</p>\n<p>Más allá de servir de soporte a criptomonedas y libros de cuentas distribuidos, el carácter <strong>descentralizado</strong> e <strong>inmutable</strong> de la tecnología <em>blockchain</em> podría dar un giro a todos aquellos modelos de negocio basados en la existencia de autoridades centrales de confianza o intermediadores.</p>\n<h3>¿Cómo puedo participar? (I) Invirtiendo/especulando</h3>\n<p>El Bitcoin y el resto de criptomonedas han sido activos volátiles hasta la fecha, y especialmente en el último año. La cotización del Bitcoin ha tenido un crecimiento espectacular en 2017. Llegando a multiplicar su precio por 20 en apenas un año. Por este motivo podría considerarse como una de las mayores burbujas de la historia. Y es habitual cuando se habla de <strong>inversión, especulación y burbujas</strong>, comparar con el caso histórico de la <a href=\"http://www.eleconomista.es/divisas/noticias/8805544/12/17/El-bitcoin-supera-ya-a-los-tulipanes-como-la-burbuja-mas-grande-de-la-historia.html\">burbuja de los tulipanes (1634–1637)</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/495/0*p6JdJRZ3_gpPHX81.png\"><figcaption><a href=\"http://maxpixel.freegreatpicture.com/Eth-Ethereum-Block-Chain-Black-Bitcoin-2458552\">http://maxpixel.freegreatpicture.com/Eth-Ethereum-Block-Chain-Black-Bitcoin-2458552</a></figcaption></figure><p>Sea una burbuja o no, en cualquier operación de compraventa resulta recomendable intentar diferenciar el valor de un activo de su precio actual. Y en el caso de las criptomonedas resulta muy difícil hacer una estimación de su valor como medios de pago o valor refugio. Y también resulta muy complejo estimar el valor de la tecnología subyacente. ¿Merece la pena pagar por unidad de cómputo en la máquina virtual de Ethereum <a href=\"https://hackernoon.com/ether-purchase-power-df40a38c5a2f\">millones de veces más</a> que por una instancia en Amazon Web Services? Dependerá de los casos de uso. Además, el propio valor de la tecnología también estará relacionado con su grado de adopción ¿Podemos estimar el valor intrínseco de la red Ethereum a día de hoy? El grado de incertidumbre de cualquier estimación sería muy alto.</p>\n<p>Si decidimos comprar criptomonedas, podemos hacerlo de forma sencilla en casas de intercambio como <strong>Coinbase</strong>. Teniendo en cuenta las comisiones por compra y venta. O usos de tarjeta como medio de pago, por ejemplo.</p>\n<h3>¿Cómo puedo participar? (II) Minando</h3>\n<p>También podemos conseguir monedas a través del proceso denominado “minado”. Denominación utilizada por analogía con otros activos como el oro.</p>\n<p>La operación de minado corresponde con la resolución de una “<a href=\"https://elbitcoin.org/el-concepto-de-prueba-de-trabajo/\">prueba de trabajo</a>”. Estas pruebas de trabajo se basan generalmente en la resolución de un problema matemático de complejidad variable. En la práctica serían equivalentes a la resolución de un puzzle. Y el procedimiento de resolución es la fuerza bruta. Por este motivo, es importante conocer una estimación de nuestra capacidad de minado.</p>\n<p>Además, en la práctica debemos analizar los gastos de capital de la compra de hardware específico, si fuera el caso. Los gastos operativos, generalmente en consumo de corriente, o de infraestructuras en la nube. Y el precio actual de la criptomoneda que vayamos a minar.</p>\n<p>Por otro lado, la relación entre la capacidad de minado de nuestra infraestructura y la del resto de la red determinará el tiempo medio de minado de nuestro primer bloque. Este tiempo es aleatorio, pero debemos tener en cuenta su valor medio. Porque nuestro retorno de inversión puede no llegar en forma de pequeños incrementos diarios, sino de una gran cantidad conseguida tras un largo periodo sin conseguir nada. Por ese motivo puede resultar más recomendable unirse a un grupo más grande de mineros, para diversificar riesgo y beneficio. Estos grupos pueden ser muy grandes, y se conocen como un <em>pool</em> de minado. En este caso riesgo y beneficio están más repartidos, y son más previsibles. A cambio se deben tener en cuenta las comisiones por pertenencia a este <em>pool</em>.</p>\n<p>Existen webs que facilitan la estimación de beneficios en base a todas estas variables, como <a href=\"https://www.cryptocompare.com/mining/calculator/eth?HashingPower=20&amp;HashingUnit=MH%2Fs&amp;PowerConsumption=140&amp;CostPerkWh=0.12&amp;MiningPoolFee=0\">cryptocompare.com</a> o <a href=\"https://www.coinwarz.com/calculators/\">coinwarz.com</a></p>\n<p><em>Fuente imagen principal: </em><a href=\"https://pixabay.com/es/criptomoneda-negocio-las-finanzas-3085139/\"><em>https://pixabay.com/es/criptomoneda-negocio-las-finanzas-3085139/</em></a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e2f8a05910f9\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/blockchain-en-5-minutos-parte-3-criptomonedas-e2f8a05910f9\">Blockchain en 5 minutos. Parte 3: Criptomonedas</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "criptomonedas",
            "bitcoin",
            "knowledge",
            "beeva-labs",
            "blockchain"
        ],
        "area": "insigths",
        "active": true,
        "id": "_m4caknd54"
    },
    {
        "title": "Voice Interactions Playbook (I)",
        "pubDate": "2017-12-11 08:04:17",
        "link": "https://labs.beeva.com/voice-interactions-playbook-i-f8c7afe56d4f?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/f8c7afe56d4f",
        "author": "Jorge Andrés Martínez | BEEVA",
        "thumbnail": "https://cdn-images-1.medium.com/max/1024/1*fec5Hd-QmmKg_DCZBEQsgw.png",
        "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fec5Hd-QmmKg_DCZBEQsgw.png\"></figure><h4>El enfoque de la interacción humano-computadora (HCI) se está trasladando mucho más allá del ratón, el tap y las interfaces visuales.</h4>\n<p><strong>El futuro de la tecnología pasa por entender cómo se puede transformar la experiencia que los humanos tienen con las máquinas a través de las nuevas interfaces</strong>.</p>\n<p>Para lograr este entendimiento con la tecnología, deberemos lograr tener una conversación fluida (como ocurre en una interacción entre humanos), en lugar de dos monólogos (como ocurre en la HCI actualmente).</p>\n<p>En este sentido, los VUIs o interfaces de voz jugarán un papel importante en el punto de encuentro entre usuario y producto, ya sea como una apoyo o incluso como un reemplazo completo a los interfaces visuales tradicionales.</p>\n<p><em>El avance en los últimos años de la interacción por voz, sobre todo en nuestros asistentes personales, precisa que los diseñadores entendamos los rasgos principales que caracterizan a estos interfaces invisibles.</em></p>\n<h4>Existe un interfaz invisible</h4>\n<p>Los interfaces de usuario de voz (<strong>VUIs</strong>) permiten al usuario interactuar con un sistema a través de comandos de voz o de habla. Algunos ejemplos de asistentes de habla virtuales son Siri, Google Assistant y Alexa.</p>\n<p><strong>La ventaja principal de un VUI es que permite que los usuarios puedan interactuar con un producto mientras se centra su atención en otra parte a modo de manos libres</strong>. El ejemplo más común es la interacción por voz con interfaces en vehículos particulares para activar comandos mientras se conduce pero, como he comentado, se están empezando a implementar en nuestros ecosistemas digitales.</p>\n<p>Como ya he comentado la comunicación de voz normalmente está asociada a la comunicación interpersonal, en lugar de la <strong>HCI</strong>. Por lo tanto, para que un <strong>VUI</strong> tenga éxito, no sólo depende del <strong>NLP</strong>, también tiene que informar a los usuarios de qué tipo de comandos de voz que pueden utilizar y de qué tipo de interacciones que pueden realizar, se trata de facilitar la imagen mental del interfaz invisible.</p>\n<p>Para entender por qué se trata de un interfaz invisible os planteo un ejemplo: la forma en la que interactuamos ante la caja de un Burger King.</p>\n<p>Tenemos a un cajero uniformado (nuestro asistente) al que podemos preguntar y un apoyo de un interfaz visual (cartelería)o una app para seleccionar productos. Si nos encontrásemos ante la misma situación pero el cajero no fuese uniformado y no dispusiéramos de apoyo de interfaces gráfico, el interfaz sería invisible y sólo tendríamos el asistente para empezar a interactuar.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*0A3LmqVFjjoTIBaQML0Ymg.png\"><figcaption>Crazy Eights realizado en taller de co-creación en BEEVA</figcaption></figure><h4>Olvida la abstracción del mundo real</h4>\n<p>Lo primero que nos damos cuenta cuando nos enfrentamos a la interacción por voz, es que falta la abstracción del mundo real que sí sucede cuando interactuamos con dispositivos 2D, por lo tanto <strong>diseñar VUIs con los mismos patrones que interfaces gráficos sería un error y por otra parte una tarea imposible.</strong></p>\n<p>En una VUI, no existen ninguna <em>affordance </em>visual. <strong>Los usuarios no tienen indicaciones claras de lo que la interfaz puede hacer</strong> o cuáles son sus opciones, por lo que <strong>es importante que el sistema limite la información que proporciona</strong>, <strong>indique claramente posibles opciones de interacción</strong> <strong>y transmita al usuario qué funcionalidad está utilizando</strong>.</p>\n<p>Todo ello enfocado a eliminar carga cognitiva y que los usuarios puedan recordar.</p>\n<h4>La importancia del contexto</h4>\n<p>Hay que tener en cuenta que al tratarse de interfaces que utilizamos habitualmente en espacios físicos cuando realizamos otras tareas, existen multitud de factores que influyen en la interacción.</p>\n<p>En cuanto a los factores propios del usuario, <strong>su estado físico</strong> (sentado, de pie, tumbado, conduciendo) , <strong>su estado emocional</strong>, <strong>su tono de voz</strong>, <strong>su sintaxis</strong> o acentos propios o <strong>el recuerdo de los comandos del interfaz</strong> son solo algunos de los factores a tener en cuenta.</p>\n<p>Por otra parte, existen otros factores ajenos al usuario como el propio espacio físico, la fecha y la hora del día, el estado de conexión, el número de dispositivos o los niveles de ruido del ambiente.</p>\n<p>Continuará…</p>\n<p>J</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f8c7afe56d4f\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/voice-interactions-playbook-i-f8c7afe56d4f\">Voice Interactions Playbook (I)</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fec5Hd-QmmKg_DCZBEQsgw.png\"></figure><h4>El enfoque de la interacción humano-computadora (HCI) se está trasladando mucho más allá del ratón, el tap y las interfaces visuales.</h4>\n<p><strong>El futuro de la tecnología pasa por entender cómo se puede transformar la experiencia que los humanos tienen con las máquinas a través de las nuevas interfaces</strong>.</p>\n<p>Para lograr este entendimiento con la tecnología, deberemos lograr tener una conversación fluida (como ocurre en una interacción entre humanos), en lugar de dos monólogos (como ocurre en la HCI actualmente).</p>\n<p>En este sentido, los VUIs o interfaces de voz jugarán un papel importante en el punto de encuentro entre usuario y producto, ya sea como una apoyo o incluso como un reemplazo completo a los interfaces visuales tradicionales.</p>\n<p><em>El avance en los últimos años de la interacción por voz, sobre todo en nuestros asistentes personales, precisa que los diseñadores entendamos los rasgos principales que caracterizan a estos interfaces invisibles.</em></p>\n<h4>Existe un interfaz invisible</h4>\n<p>Los interfaces de usuario de voz (<strong>VUIs</strong>) permiten al usuario interactuar con un sistema a través de comandos de voz o de habla. Algunos ejemplos de asistentes de habla virtuales son Siri, Google Assistant y Alexa.</p>\n<p><strong>La ventaja principal de un VUI es que permite que los usuarios puedan interactuar con un producto mientras se centra su atención en otra parte a modo de manos libres</strong>. El ejemplo más común es la interacción por voz con interfaces en vehículos particulares para activar comandos mientras se conduce pero, como he comentado, se están empezando a implementar en nuestros ecosistemas digitales.</p>\n<p>Como ya he comentado la comunicación de voz normalmente está asociada a la comunicación interpersonal, en lugar de la <strong>HCI</strong>. Por lo tanto, para que un <strong>VUI</strong> tenga éxito, no sólo depende del <strong>NLP</strong>, también tiene que informar a los usuarios de qué tipo de comandos de voz que pueden utilizar y de qué tipo de interacciones que pueden realizar, se trata de facilitar la imagen mental del interfaz invisible.</p>\n<p>Para entender por qué se trata de un interfaz invisible os planteo un ejemplo: la forma en la que interactuamos ante la caja de un Burger King.</p>\n<p>Tenemos a un cajero uniformado (nuestro asistente) al que podemos preguntar y un apoyo de un interfaz visual (cartelería)o una app para seleccionar productos. Si nos encontrásemos ante la misma situación pero el cajero no fuese uniformado y no dispusiéramos de apoyo de interfaces gráfico, el interfaz sería invisible y sólo tendríamos el asistente para empezar a interactuar.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*0A3LmqVFjjoTIBaQML0Ymg.png\"><figcaption>Crazy Eights realizado en taller de co-creación en BEEVA</figcaption></figure><h4>Olvida la abstracción del mundo real</h4>\n<p>Lo primero que nos damos cuenta cuando nos enfrentamos a la interacción por voz, es que falta la abstracción del mundo real que sí sucede cuando interactuamos con dispositivos 2D, por lo tanto <strong>diseñar VUIs con los mismos patrones que interfaces gráficos sería un error y por otra parte una tarea imposible.</strong></p>\n<p>En una VUI, no existen ninguna <em>affordance </em>visual. <strong>Los usuarios no tienen indicaciones claras de lo que la interfaz puede hacer</strong> o cuáles son sus opciones, por lo que <strong>es importante que el sistema limite la información que proporciona</strong>, <strong>indique claramente posibles opciones de interacción</strong> <strong>y transmita al usuario qué funcionalidad está utilizando</strong>.</p>\n<p>Todo ello enfocado a eliminar carga cognitiva y que los usuarios puedan recordar.</p>\n<h4>La importancia del contexto</h4>\n<p>Hay que tener en cuenta que al tratarse de interfaces que utilizamos habitualmente en espacios físicos cuando realizamos otras tareas, existen multitud de factores que influyen en la interacción.</p>\n<p>En cuanto a los factores propios del usuario, <strong>su estado físico</strong> (sentado, de pie, tumbado, conduciendo) , <strong>su estado emocional</strong>, <strong>su tono de voz</strong>, <strong>su sintaxis</strong> o acentos propios o <strong>el recuerdo de los comandos del interfaz</strong> son solo algunos de los factores a tener en cuenta.</p>\n<p>Por otra parte, existen otros factores ajenos al usuario como el propio espacio físico, la fecha y la hora del día, el estado de conexión, el número de dispositivos o los niveles de ruido del ambiente.</p>\n<p>Continuará…</p>\n<p>J</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f8c7afe56d4f\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/voice-interactions-playbook-i-f8c7afe56d4f\">Voice Interactions Playbook (I)</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "report",
            "hci",
            "voice-interfaces",
            "interaction-design",
            "beeva-labs"
        ],
        "area": "insigths",
        "active": true,
        "id": "_u66ghublf"
    },
    {
        "title": "Chameleon hashes y actualizaciones de firmware",
        "pubDate": "2017-11-20 08:33:01",
        "link": "https://labs.beeva.com/chameleon-hashes-y-actualizaciones-de-firmware-ba3e1f2be16f?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/ba3e1f2be16f",
        "author": "Jesús Diaz Vico | BEEVA",
        "thumbnail": "https://cdn-images-1.medium.com/max/730/1*lCn5PEvfj21-wcIhILl6NA.jpeg",
        "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*lCn5PEvfj21-wcIhILl6NA.jpeg\"></figure><p>En BBVA estamos comprometidos con la renovación tecnológica. Esto afecta especialmente tanto al departamento de New Digital Businesses — Technology, como a <a href=\"https://www.beeva.com/\">BEEVA</a>, empresa del grupo con foco en tecnología e innovación. Fruto de este esfuerzo, estamos muy orgullosos de haber enviado una de nuestras primeras solicitudes de patente a la EPO (European Patents Office). Vamos a ver de qué se trata.</p>\n<h4>Contexto</h4>\n<p>El foco de nuestra aplicación de patente es en el arranque seguro de dispositivos. En este contexto, hay mucho trabajo previo y sistemas que ya están en producción y con un despliegue masivo. Por ejemplo, el <a href=\"https://technet.microsoft.com/en-us/library/hh824987.aspx\"><em>Secure Boot</em></a><em> </em>de UEFI y el <a href=\"https://source.android.com/security/verifiedboot/\"><em>Verified Boot</em></a> de dispositivos Android. Nosotros nos centramos en estos últimos.</p>\n<p>Estos procesos de arranque seguro funcionan bastante bien y, aunque son bastante rápidos, pasan muchas cosas sin que nos demos cuenta. La parte más importante, encargada de asegurarse de que el software que se carga es legítimo, comprueba criptográficamente que cada pieza de software viene de una fuente fiable. En dispositivos Android, se ejecuta una cascada de verificaciones de firmas digitales, usando como origen de confianza una clave pública “raíz” que está escrita en la zona de sólo lectura. La siguiente figura resume este proceso.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Pt8WECcesHY5Xfrc.\"><figcaption>Cascada de verificaciones en dispositivos derivados de Android (imagen del proyecto <a href=\"https://www.chromium.org/chromium-os/chromiumos-design-docs/verified-boot-data-structures\">Chromium</a>).</figcaption></figure><h4>Entonces, ¿cuál es el problema?</h4>\n<p>Este proceso hace un buen trabajo en asegurar la integridad del software (firmware y sistema operativo) que se carga. Pero en algunos casos, puede ser bastante tedioso. En concreto, si quieres aplicar un cambio sobre el firmware, el sistema operativo, o un driver del mismo, necesitas obtener una nueva firma digital con una clave que no controlas: la clave raíz que hemos comentado antes, o alguna de las siguientes en la “cascada”.</p>\n<p>Por ejemplo, y usando las estructuras y claves indicadas en la figura anterior, pensemos en una empresa que se encarga de mantener un driver para algún componente específico, como el acelerómetro de un móvil. Sus desarrolladores encuentran y resuelven un bug en dicho driver y quieren desplegar la nueva versión. Para ello, necesitarán que la nueva versión de su driver se incluya en el <em>kernel body. </em>Al cambiar el <em>kernel body</em>, es necesario emitir una nueva firma del mismo (una nueva <em>kernel body sig</em>) para que el proceso de <em>verified boot</em> se complete con éxito. Pero esta firma se emite con la <em>kernel data key</em>, que a su vez está firmada por la <em>kernel subkey</em> y está última por la <em>firmware data key, </em>que ya viene firmada por la <em>root key</em>. Probablemente, ninguna de estas claves esté bajo tu control (sino más bien del de la empresa de desarrollo del kernel, firmware, o incluso del mismo OEM en dispositivos restringidos). En conclusión: tienes una versión más segura de tu driver, pero probablemente no puedas desplegarla hasta que encaje en el ciclo de trabajo de otra empresa.</p>\n<h4>Nuestra propuesta: <em>chameleon hashes</em>\n</h4>\n<p>Como acabamos de describir, el cuello de botella es la obtención de firmas digitales por parte de la autoridad competente. Una posible estrategia (la nuestra, de hecho), es precisamente “delegar” esta tarea a entidades autorizadas. Por supuesto, no podemos simplemente andar distribuyendo copias de las claves privadas, por mucho que sea a entidades autorizadas. Probablemente nadie aceptaría un sistema así, además de que es una muy mala práctica de seguridad. Pero entonces, ¿cómo generamos nuevas firmas digitales sobre una versión modificada de un driver o de un firmware? Nuestra idea es precisamente evitar que haya que generar estas nuevas firmas digitales, incluso si el contenido que protegen ha cambiado.</p>\n<p>Llegados a este punto, quienes sepan algo de firmas digitales, pensarán que esto no tiene sentido. Y es aquí precisamente donde entra en juego una primitiva criptográfica muy interesante: los <a href=\"https://eprint.iacr.org/2004/243.pdf\"><em>chameleon hashes</em></a> (a veces también llamados <em>chameleon signatures</em>).</p>\n<p>Por un lado, un chameleon hash es como un hash criptográfico convencional, en el sentido de que recibe una cadena de bits de longitud variable y produce, de forma determinista, una cadena de bits aleatoria y de longitud fija*.</p>\n<p>Por otro lado, tiene diferencias fundamentales: es una función hash que requiere clave, al contrario que las tradicionales funciones hash criptográficas. En particular, para calcular un chameleon hash es necesario el propio mensaje a hashear y una clave pública. Además, como salida de un chameleon hash, se produce un valor auxiliar que luego servirá para comprobar el hash propiamente dicho. Una vez calculado de esta forma, el propietario de la clave privada correspondiente (y nadie más) podrá encontrar colisiones arbitrarias sobre ese valor hash. Es decir, si tienes un chameleon hash <em>ch</em>, con valor auxiliar <em>aux</em>, calculado sobre un mensaje <em>msg</em> con la clave pública <em>pk</em>, entonces el propietario de la clave privada asociada, <em>sk</em>, podrá hacer que cualquier mensaje <em>msg’ </em>tenga también un chameleon hash con valor <em>ch</em>, aunque el valor auxiliar cambiaría a otro <em>aux’</em>. La siguiente figura resume las funciones asociadas.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/769/1*Ro8jSpZk3SnHNzwy013r0g.png\"><figcaption>Diagramas de las funciones de un chameleon hash.</figcaption></figure><p>Volviendo a las firmas digitales, para firmar un mensaje, lo normal es hashearlo primero y aplicar el algoritmo de firma después. En los hashes criptográficos tradicionales, con modificar un único bit de la entrada, cambia aproximadamente el 50% de los bits de la salida, pero usando chameleon hashes en vez de hashes tradicionales, podemos mantener la misma salida para distintas entradas, si tenemos la clave privada necesaria.</p>\n<h4>Una primitiva más: los árboles Merkle</h4>\n<p>Hay un aspecto más al que le sacamos provecho en dispositivos derivados de Android. Como se muestra en la siguiente figura, en Android, los datos que se almacenan en el sistema de ficheros, incluyendo módulos del kernel, se dividen en bloques. Estos bloques componen las hojas de un <a href=\"https://en.wikipedia.org/wiki/Merkle_tree\">árbol Merkle</a>, cuya raíz se incluye en una de las estructuras de datos protegidas con firmas digitalmente con la clave de firma del kernel (la <em>kernel data key </em>de la primera figura).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/560/0*mNF9GdGvBnBGL72Q.\"><figcaption>La estructura de árbol Merkle usada para proteger bloques en Android (imagen de la web sobre <a href=\"https://source.android.com/security/verifiedboot/\"><em>Verified Boot</em></a><em>)</em>.</figcaption></figure><p>Ahora supongamos que cada uno de estos bloques (o conjunto de bloques) se asigna a un driver del kernel, mantenido a su vez por una empresa de desarrollo distinta. Cada una de estas empresas tiene, además, un par de claves para calcular chameleon hashes, autorizado por la <em>firmware data key </em>o por la <em>kernel data key</em>. Esta autorización no implicaría (por ejemplo) más que escribir en alguno de los preámbulos anteriores la clave pública del par de claves, firmada con la <em>root key, </em>la <em>firmware data key</em> o la <em>kernel data key</em>, según queramos.</p>\n<p>Entonces, cada empresa escribirá en su bloque (o bloques) correspondiente(s) el chameleon hash de su driver. Todos estos chameleon hashes se combinan construyendo el árbol Merkle igual que antes, hasta dar con la raíz, que se escribe en el <em>kernel preamble</em>. Si posteriormente una de estas empresas produce una nueva versión de su driver, simplemente tendrá que usar su clave privada para encontrar una colisión del chameleon hash que escribió en su bloque, por lo que su bloque seguirá siendo el mismo. Como el resto de bloques tampoco se modifican, la raíz del árbol Merkle sigue siendo la misma, así que no es necesario re-emitir la firma digital del kernel. Además, como sólo el propietario de la clave privada usada para calcular el chameleon hash puede obtener colisiones (es decir, la empresa encargada del driver, que ha sido autorizada previamente), ninguna otra persona o entidad podrá hacer este proceso, con lo que también mantenemos la “cadena de integridad”.</p>\n<h4>Un ejemplo “de juguete”</h4>\n<p>Tomemos la siguiente figura como base:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/679/0*T4uUsfcJTVsFGUab.\"><figcaption>Un árbol Merkle con chameleon hashes por hojas.</figcaption></figure><p>En ella, vemos un sistema simplificado compuesto por cuatro drivers: uno para GPS, otro para el acelerómetro, otro para la pantalla y el último para el teclado. Cada uno de estos drivers lo mantiene una empresa distinta, con su propio par de claves (siendo <em>pkGPS</em> la clave pública de la empresa encargada del driver del GPS, etc.).</p>\n<p>Al fabricar el dispositivo, el OEM incluye las claves públicas de estas compañías en alguna de las estructuras de datos firmadas (directa o indirectamente) por la <em>root key</em>. Supongamos que lo hace en el <em>kernel preamble</em>. Además, escribe en los bloques asociados a cada driver la última versión de los mismos. A partir de estos bloques (los datos del driver del GPS son <em>0x2746</em> en el ejemplo, etc.) y usando la clave pública correspondiente, calcula los chameleon hashes. Por ejemplo, con la clave <em>pkGPS </em>y el driver del GPS, obtiene el chameleon hash <em>0x0123</em>**. Toda la información utilizada es pública o accesible para el OEM, así que no tiene problema en hacerlo. Este mismo proceso se repite para los 3 drivers restantes, produciendo chameleon hashes con valores <em>0x4567</em>, <em>0x89AB</em> y <em>0xCDEF</em>. Estos hashes serán las hojas del árbol Merkle, que se van combinando hasta producir la raíz con valor <em>0x3333 </em>(nótese cómo para calcular los nodos intermedios se usan hashes criptográficos tradicionales). Este “root hash” es el que se escribe en el preámbulo del kernel, y el que se firma con la <em>kernel data key</em>.</p>\n<p>En algún momento, la compañía de GPS crea una nueva versión de su driver, con valor <em>0x2747</em>, arreglando importantes bugs. Como la compañía controla la clave privada <em>skGPS</em> asociada a la clave pública <em>pkGPS</em>, puede ejecutar la función de cómputo de colisiones asociada al chameleon hash, lo que hará que el valor <em>0x0123 </em>siga siendo un chameleon hash válido para la nueva versión con valor <em>0x2747</em>. Es decir, las hojas del árbol Merkle seguirán siendo 0x0123, <em>0x4567</em>, <em>0x89AB</em> y <em>0xCDEF</em>, lo que producirá la misma raíz, <em>0x3333</em> y, finalmente, no cambiará el <em>kernel preamble</em>. Exactamente lo que queríamos para evitar pedir al OEM que emita una nueva firma digital.</p>\n<p><em>Fuente de la imagen de cabecera: </em><a href=\"https://vimeo.com/205270728\"><em>vimeo</em></a>.</p>\n<p>* Estrictamente, en este caso la longitud de la salida no es necesariamente fija, pero sí “aproximadamente del mismo tamaño”.</p>\n<p>** Los valores auxiliares habría que almacenarlos en otro sitio. Para simplificar la explicación, los obviamos aquí y asumimos simplemente que están disponibles para cualquiera.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ba3e1f2be16f\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/chameleon-hashes-y-actualizaciones-de-firmware-ba3e1f2be16f\">Chameleon hashes y actualizaciones de firmware</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*lCn5PEvfj21-wcIhILl6NA.jpeg\"></figure><p>En BBVA estamos comprometidos con la renovación tecnológica. Esto afecta especialmente tanto al departamento de New Digital Businesses — Technology, como a <a href=\"https://www.beeva.com/\">BEEVA</a>, empresa del grupo con foco en tecnología e innovación. Fruto de este esfuerzo, estamos muy orgullosos de haber enviado una de nuestras primeras solicitudes de patente a la EPO (European Patents Office). Vamos a ver de qué se trata.</p>\n<h4>Contexto</h4>\n<p>El foco de nuestra aplicación de patente es en el arranque seguro de dispositivos. En este contexto, hay mucho trabajo previo y sistemas que ya están en producción y con un despliegue masivo. Por ejemplo, el <a href=\"https://technet.microsoft.com/en-us/library/hh824987.aspx\"><em>Secure Boot</em></a><em> </em>de UEFI y el <a href=\"https://source.android.com/security/verifiedboot/\"><em>Verified Boot</em></a> de dispositivos Android. Nosotros nos centramos en estos últimos.</p>\n<p>Estos procesos de arranque seguro funcionan bastante bien y, aunque son bastante rápidos, pasan muchas cosas sin que nos demos cuenta. La parte más importante, encargada de asegurarse de que el software que se carga es legítimo, comprueba criptográficamente que cada pieza de software viene de una fuente fiable. En dispositivos Android, se ejecuta una cascada de verificaciones de firmas digitales, usando como origen de confianza una clave pública “raíz” que está escrita en la zona de sólo lectura. La siguiente figura resume este proceso.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Pt8WECcesHY5Xfrc.\"><figcaption>Cascada de verificaciones en dispositivos derivados de Android (imagen del proyecto <a href=\"https://www.chromium.org/chromium-os/chromiumos-design-docs/verified-boot-data-structures\">Chromium</a>).</figcaption></figure><h4>Entonces, ¿cuál es el problema?</h4>\n<p>Este proceso hace un buen trabajo en asegurar la integridad del software (firmware y sistema operativo) que se carga. Pero en algunos casos, puede ser bastante tedioso. En concreto, si quieres aplicar un cambio sobre el firmware, el sistema operativo, o un driver del mismo, necesitas obtener una nueva firma digital con una clave que no controlas: la clave raíz que hemos comentado antes, o alguna de las siguientes en la “cascada”.</p>\n<p>Por ejemplo, y usando las estructuras y claves indicadas en la figura anterior, pensemos en una empresa que se encarga de mantener un driver para algún componente específico, como el acelerómetro de un móvil. Sus desarrolladores encuentran y resuelven un bug en dicho driver y quieren desplegar la nueva versión. Para ello, necesitarán que la nueva versión de su driver se incluya en el <em>kernel body. </em>Al cambiar el <em>kernel body</em>, es necesario emitir una nueva firma del mismo (una nueva <em>kernel body sig</em>) para que el proceso de <em>verified boot</em> se complete con éxito. Pero esta firma se emite con la <em>kernel data key</em>, que a su vez está firmada por la <em>kernel subkey</em> y está última por la <em>firmware data key, </em>que ya viene firmada por la <em>root key</em>. Probablemente, ninguna de estas claves esté bajo tu control (sino más bien del de la empresa de desarrollo del kernel, firmware, o incluso del mismo OEM en dispositivos restringidos). En conclusión: tienes una versión más segura de tu driver, pero probablemente no puedas desplegarla hasta que encaje en el ciclo de trabajo de otra empresa.</p>\n<h4>Nuestra propuesta: <em>chameleon hashes</em>\n</h4>\n<p>Como acabamos de describir, el cuello de botella es la obtención de firmas digitales por parte de la autoridad competente. Una posible estrategia (la nuestra, de hecho), es precisamente “delegar” esta tarea a entidades autorizadas. Por supuesto, no podemos simplemente andar distribuyendo copias de las claves privadas, por mucho que sea a entidades autorizadas. Probablemente nadie aceptaría un sistema así, además de que es una muy mala práctica de seguridad. Pero entonces, ¿cómo generamos nuevas firmas digitales sobre una versión modificada de un driver o de un firmware? Nuestra idea es precisamente evitar que haya que generar estas nuevas firmas digitales, incluso si el contenido que protegen ha cambiado.</p>\n<p>Llegados a este punto, quienes sepan algo de firmas digitales, pensarán que esto no tiene sentido. Y es aquí precisamente donde entra en juego una primitiva criptográfica muy interesante: los <a href=\"https://eprint.iacr.org/2004/243.pdf\"><em>chameleon hashes</em></a> (a veces también llamados <em>chameleon signatures</em>).</p>\n<p>Por un lado, un chameleon hash es como un hash criptográfico convencional, en el sentido de que recibe una cadena de bits de longitud variable y produce, de forma determinista, una cadena de bits aleatoria y de longitud fija*.</p>\n<p>Por otro lado, tiene diferencias fundamentales: es una función hash que requiere clave, al contrario que las tradicionales funciones hash criptográficas. En particular, para calcular un chameleon hash es necesario el propio mensaje a hashear y una clave pública. Además, como salida de un chameleon hash, se produce un valor auxiliar que luego servirá para comprobar el hash propiamente dicho. Una vez calculado de esta forma, el propietario de la clave privada correspondiente (y nadie más) podrá encontrar colisiones arbitrarias sobre ese valor hash. Es decir, si tienes un chameleon hash <em>ch</em>, con valor auxiliar <em>aux</em>, calculado sobre un mensaje <em>msg</em> con la clave pública <em>pk</em>, entonces el propietario de la clave privada asociada, <em>sk</em>, podrá hacer que cualquier mensaje <em>msg’ </em>tenga también un chameleon hash con valor <em>ch</em>, aunque el valor auxiliar cambiaría a otro <em>aux’</em>. La siguiente figura resume las funciones asociadas.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/769/1*Ro8jSpZk3SnHNzwy013r0g.png\"><figcaption>Diagramas de las funciones de un chameleon hash.</figcaption></figure><p>Volviendo a las firmas digitales, para firmar un mensaje, lo normal es hashearlo primero y aplicar el algoritmo de firma después. En los hashes criptográficos tradicionales, con modificar un único bit de la entrada, cambia aproximadamente el 50% de los bits de la salida, pero usando chameleon hashes en vez de hashes tradicionales, podemos mantener la misma salida para distintas entradas, si tenemos la clave privada necesaria.</p>\n<h4>Una primitiva más: los árboles Merkle</h4>\n<p>Hay un aspecto más al que le sacamos provecho en dispositivos derivados de Android. Como se muestra en la siguiente figura, en Android, los datos que se almacenan en el sistema de ficheros, incluyendo módulos del kernel, se dividen en bloques. Estos bloques componen las hojas de un <a href=\"https://en.wikipedia.org/wiki/Merkle_tree\">árbol Merkle</a>, cuya raíz se incluye en una de las estructuras de datos protegidas con firmas digitalmente con la clave de firma del kernel (la <em>kernel data key </em>de la primera figura).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/560/0*mNF9GdGvBnBGL72Q.\"><figcaption>La estructura de árbol Merkle usada para proteger bloques en Android (imagen de la web sobre <a href=\"https://source.android.com/security/verifiedboot/\"><em>Verified Boot</em></a><em>)</em>.</figcaption></figure><p>Ahora supongamos que cada uno de estos bloques (o conjunto de bloques) se asigna a un driver del kernel, mantenido a su vez por una empresa de desarrollo distinta. Cada una de estas empresas tiene, además, un par de claves para calcular chameleon hashes, autorizado por la <em>firmware data key </em>o por la <em>kernel data key</em>. Esta autorización no implicaría (por ejemplo) más que escribir en alguno de los preámbulos anteriores la clave pública del par de claves, firmada con la <em>root key, </em>la <em>firmware data key</em> o la <em>kernel data key</em>, según queramos.</p>\n<p>Entonces, cada empresa escribirá en su bloque (o bloques) correspondiente(s) el chameleon hash de su driver. Todos estos chameleon hashes se combinan construyendo el árbol Merkle igual que antes, hasta dar con la raíz, que se escribe en el <em>kernel preamble</em>. Si posteriormente una de estas empresas produce una nueva versión de su driver, simplemente tendrá que usar su clave privada para encontrar una colisión del chameleon hash que escribió en su bloque, por lo que su bloque seguirá siendo el mismo. Como el resto de bloques tampoco se modifican, la raíz del árbol Merkle sigue siendo la misma, así que no es necesario re-emitir la firma digital del kernel. Además, como sólo el propietario de la clave privada usada para calcular el chameleon hash puede obtener colisiones (es decir, la empresa encargada del driver, que ha sido autorizada previamente), ninguna otra persona o entidad podrá hacer este proceso, con lo que también mantenemos la “cadena de integridad”.</p>\n<h4>Un ejemplo “de juguete”</h4>\n<p>Tomemos la siguiente figura como base:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/679/0*T4uUsfcJTVsFGUab.\"><figcaption>Un árbol Merkle con chameleon hashes por hojas.</figcaption></figure><p>En ella, vemos un sistema simplificado compuesto por cuatro drivers: uno para GPS, otro para el acelerómetro, otro para la pantalla y el último para el teclado. Cada uno de estos drivers lo mantiene una empresa distinta, con su propio par de claves (siendo <em>pkGPS</em> la clave pública de la empresa encargada del driver del GPS, etc.).</p>\n<p>Al fabricar el dispositivo, el OEM incluye las claves públicas de estas compañías en alguna de las estructuras de datos firmadas (directa o indirectamente) por la <em>root key</em>. Supongamos que lo hace en el <em>kernel preamble</em>. Además, escribe en los bloques asociados a cada driver la última versión de los mismos. A partir de estos bloques (los datos del driver del GPS son <em>0x2746</em> en el ejemplo, etc.) y usando la clave pública correspondiente, calcula los chameleon hashes. Por ejemplo, con la clave <em>pkGPS </em>y el driver del GPS, obtiene el chameleon hash <em>0x0123</em>**. Toda la información utilizada es pública o accesible para el OEM, así que no tiene problema en hacerlo. Este mismo proceso se repite para los 3 drivers restantes, produciendo chameleon hashes con valores <em>0x4567</em>, <em>0x89AB</em> y <em>0xCDEF</em>. Estos hashes serán las hojas del árbol Merkle, que se van combinando hasta producir la raíz con valor <em>0x3333 </em>(nótese cómo para calcular los nodos intermedios se usan hashes criptográficos tradicionales). Este “root hash” es el que se escribe en el preámbulo del kernel, y el que se firma con la <em>kernel data key</em>.</p>\n<p>En algún momento, la compañía de GPS crea una nueva versión de su driver, con valor <em>0x2747</em>, arreglando importantes bugs. Como la compañía controla la clave privada <em>skGPS</em> asociada a la clave pública <em>pkGPS</em>, puede ejecutar la función de cómputo de colisiones asociada al chameleon hash, lo que hará que el valor <em>0x0123 </em>siga siendo un chameleon hash válido para la nueva versión con valor <em>0x2747</em>. Es decir, las hojas del árbol Merkle seguirán siendo 0x0123, <em>0x4567</em>, <em>0x89AB</em> y <em>0xCDEF</em>, lo que producirá la misma raíz, <em>0x3333</em> y, finalmente, no cambiará el <em>kernel preamble</em>. Exactamente lo que queríamos para evitar pedir al OEM que emita una nueva firma digital.</p>\n<p><em>Fuente de la imagen de cabecera: </em><a href=\"https://vimeo.com/205270728\"><em>vimeo</em></a>.</p>\n<p>* Estrictamente, en este caso la longitud de la salida no es necesariamente fija, pero sí “aproximadamente del mismo tamaño”.</p>\n<p>** Los valores auxiliares habría que almacenarlos en otro sitio. Para simplificar la explicación, los obviamos aquí y asumimos simplemente que están disponibles para cualquiera.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ba3e1f2be16f\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/chameleon-hashes-y-actualizaciones-de-firmware-ba3e1f2be16f\">Chameleon hashes y actualizaciones de firmware</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "iot",
            "innovación",
            "criptografia"
        ],
        "area": "insigths",
        "active": true,
        "id": "_6dg3gwh9s"
    },
    {
        "title": "OCR for password detection in video frames",
        "pubDate": "2017-10-31 12:52:35",
        "link": "https://labs.beeva.com/ocr-for-password-detection-in-video-frames-ff8c1cb4a2f0?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/ff8c1cb4a2f0",
        "author": "Luis Enrique Mesa Salas | BEEVA",
        "thumbnail": "https://cdn-images-1.medium.com/max/730/1*dixc3zIwRaOkzQS-BIngDA.jpeg",
        "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*dixc3zIwRaOkzQS-BIngDA.jpeg\"></figure><p>By mistake, a password or an access token could be displayed on an <a href=\"https://en.wikipedia.org/wiki/Proof_of_concept\">proof of concept</a> (PoC), demo or course. If this session is recorded and uploaded to the internet, this could be a major security issue. It would be helpful to detect these issues before sharing those videos to the world.</p>\n<p>We realized, at <a href=\"https://www.beeva.com/\">BEEVA</a>, that we could do that by using an OCR software. So we decided to do a PoC focused on evaluating several OCR tools capabilities to extract text from video frames. If we could extract text with acceptable quality, could we identify any password?</p>\n<p>Based on a set of videos of different kind (in local filesystem or uploaded to YouTube) we have made several tests for our purpose. In this post, we’re going to show strengths, weaknesses, cost, development effort, etc. of each OCR software analyzed.</p>\n<h3>Tesseract OCR</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/626/1*_45jJkVSEcJZh1nhbMcFqA.png\"><figcaption>Tesseract-OCR logo</figcaption></figure><p><a href=\"https://github.com/tesseract-ocr/tesseract/wiki\">Tesseract</a> is a free OCR software sponsored by Google since 2006. It’s considered one of the most accurate open-source OCR engines available. We used this OCR as a local choice and with default parameters and models.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Open source and easy to use.</li>\n<li>Supports 100 languages.</li>\n<li>Free and available on Linux, MAC and Windows (with an unofficial installer).</li>\n<li>You can train or improve models.</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Doesn’t support language recognition.</li>\n<li>Very poor image pre-processing.</li>\n<li>High software development cost.</li>\n</ul>\n<h3>OCR Space</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/188/1*vH4q_q0LzuB1RjI28Kfs8w.png\"><figcaption>OCR.Space logo</figcaption></figure><p><a href=\"https://ocr.space/\">OCR.space</a> is a service of <a href=\"https://a9t9.com/about\">a9t9 software GmbH</a>. Provides a free to use, no registration necessary API with some size/number or requests limitations. Also, they offer an improved service with several <a href=\"https://ocr.space/ocrapi#free\">price plans</a>.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Easy to use, OS independent and well documented.</li>\n<li>Supports 24 languages.</li>\n<li>Provides a limited free OCR API.</li>\n<li>They assure you they are not keeping any of your data.</li>\n<li>Very low software development cost.</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>No chance to train or improve models.</li>\n<li>Doesn’t support language recognition.</li>\n<li>Unless you have a PRO PDF plan, you have a file size upload limit.</li>\n</ul>\n<h3>Google Cloud Vision</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/626/1*UCYXS1IiFi4eldwZJjq6QQ.jpeg\"><figcaption>Google Cloud Vision API logo</figcaption></figure><p><a href=\"https://cloud.google.com/vision\">Google Cloud Vision</a> API provides powerful machine learning models in an easy to use REST API. One point of the API is an OCR Service, with some <a href=\"https://cloud.google.com/vision/pricing\">free tier</a> capabilities but registration needed.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Easy to use, OS independent and well documented.</li>\n<li>Supports 56 languages.</li>\n<li>Provides a limited free OCR API.</li>\n<li>Supports language recognition.</li>\n<li>Very low software development cost.</li>\n<li>Models are trained and improved periodically by Google and their huge development team and computing capabilities.</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>No chance to train or improve models (not a real problem, as I said before, you have Google on your back improving their models).</li>\n<li>You have a file size upload limit to 4MB per image.</li>\n</ul>\n<h3>Summary report</h3>\n<p>In this last section, we’re going to tell you some general conclusions that we have made about this PoC and some hints for choosing one OCR software according to several criteria.</p>\n<h4>General insights</h4>\n<ul>\n<li>Frames extraction time cost will be linear for same video quality.</li>\n<li>\n<strong>Frame size</strong> (not resolution) will <strong>increase</strong> the <strong>cost</strong> dramatically.</li>\n<li>Tesseract (with default configuration) takes the <strong>same average time</strong> as <strong>OCR-Space or Google Cloud Vision </strong>(including networking time).</li>\n<li>\n<strong>Noise</strong> generated by OCR<strong> </strong>could <strong>spoil password recognition </strong>(since we use regexp patterns for identifying them).</li>\n</ul>\n<h4>Pricing</h4>\n<ul>\n<li>\n<strong>Tesseract</strong>: completely <strong>free</strong>.</li>\n<li>\n<strong>OCR.Space</strong>: free tier with 25K images (1MB image max. size). <strong>PRO plan</strong> with 250K images (5MB image max. size) <strong>24.95$/month</strong>. <strong>PRO PDF plan</strong> with 250K images (no size limit) <strong>49.95$/month</strong>. In both PRO plans, you can buy additional images upload by 10$/month each 100K additonal images.</li>\n<li>\n<strong>Google Cloud Vision</strong>: free tier with 1K first images. <strong>1.5$/month per 1K units</strong> from 1001 to 5M units and <strong>0.6$/month per 1K units</strong> from 5M to 20M units.</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/579/1*tdVBbqDn0buenQ1-14Tp1Q.png\"><figcaption>Pricing comparation between Tesseract, OCR.Space and Google Cloud Vision</figcaption></figure><p><strong>Frame size limitations</strong></p>\n<ul>\n<li>\n<strong>Tesseract</strong>: <strong>32767x32767 px</strong> image, <strong>no </strong>file <strong>size limitations</strong>.</li>\n<li>\n<strong>OCR Space</strong>: 1MB/image in free tier, 5MB/image in PRO plan and <strong>no limitations in PRO PDF plan</strong>.</li>\n<li>\n<strong>Google Cloud Vision</strong>: <strong>4MB/image </strong>and a maximum of <strong>8MB/request </strong>(allowing to upload 16 images in a single request) in all billing types.</li>\n</ul>\n<p><strong>Frame number limitations</strong></p>\n<ul>\n<li>\n<strong>Tesseract</strong>: <strong>no limitations</strong>, limited only by local storage.</li>\n<li>\n<strong>OCR Space</strong>: 25K req/month and 500 calls/day in free tier<strong> </strong>and <strong>250K req/month</strong> and <strong>600 calls/min</strong> in all PRO plans (but you can buy additional loads in 100K image blocks)</li>\n<li>\n<strong>Google Cloud Vision</strong>: <strong>600 calls/min</strong>, <strong>20M images/month </strong>and 16 images/request in all billing types.</li>\n</ul>\n<p><strong>Development</strong></p>\n<ul>\n<li>\n<strong>Tesseract</strong>: You may need a <strong>strong effort</strong>, or a very good quality images, for achieving quite good results, but <strong>you can train and improve</strong> your own or previous existing <strong>models</strong> as you wish.</li>\n<li>\n<strong>OCR Space </strong>and <strong>Google Cloud Vision</strong>: very <strong>low effort</strong>. You only have to make an HTTP request module or adapter in the worst case.</li>\n</ul>\n<p><strong>Accuracy</strong></p>\n<ul>\n<li>\n<strong>Tesseract</strong>: <strong>very poor </strong>without training and only with <strong>one language texts</strong>.</li>\n<li>\n<strong>OCR Space</strong>: good or <strong>acceptable</strong> for only <strong>one language texts</strong>.</li>\n<li>\n<strong>Google Cloud Vision</strong>: <strong>awesome! </strong>and<strong> needed </strong>in<strong> multi language texts</strong>.</li>\n</ul>\n<h3>Final thoughts</h3>\n<p><strong>Tesseract</strong>, without a quite good images and without spending a bunch of development time, <strong>may be</strong> a little bit <strong>discouraged</strong>, but <strong>necessary</strong> in some projects where you have to use only your local storage <strong>due to legal clauses</strong>.</p>\n<p>On the other hand, and if you have a <strong>multilanguage</strong> text or a <strong>poor</strong> image <strong>quality</strong>, I would choose <strong>Google Cloud Vision</strong> without any doubts. <strong>OCR Space </strong>would be an interesting and <strong>cheaper</strong> option but only if you have a good enough frameset and mostly in a <strong>unique language</strong>.</p>\n<h4><em>Links</em></h4>\n<ul>\n<li><a href=\"https://docs.google.com/presentation/d/1pa1BLJcjipi5h6o4_QPFR_aF-83I03CfgTXqb56dWtE\"><em>OCR for password detection in video frames presentation</em></a></li>\n<li><a href=\"https://github.com/beeva-luismesa/beeva-poc-ocr\"><em>This PoC repository</em></a></li>\n<li>\n<a href=\"http://projectnaptha.com/\"><em>Project Naptha</em></a><em> (a Google Chrome extension to use Tesseract on images displayed on your browser)</em>\n</li>\n<li>\n<a href=\"https://chrome.google.com/webstore/detail/copyfish-%F0%9F%90%9F-free-ocr-soft/eenjdnjldapjajjofmldgmkjaienebbj\"><em>Copyfish for Google Chrome</em></a><em> (extension to use OCR.Space on images or videos displayed on your browser)</em>\n</li>\n<li>\n<a href=\"https://addons.mozilla.org/es/firefox/addon/copyfish-ocr-software/\"><em>Copyfish for Mozilla Firefox</em></a><em> (addon to use OCR.Space on images or videos displayed on your browser)</em>\n</li>\n<li><a href=\"https://cloud.google.com/vision/docs/drag-and-drop\"><em>Google Cloud Vision ‘drag and drop’ demo</em></a></li>\n</ul>\n<h4><em>Images</em></h4>\n<ul>\n<li><a href=\"https://pixabay.com/es/hacker-de-seguridad-cyber-internet-2002907/\"><em>Main picture</em></a></li>\n<li><a href=\"https://upload.wikimedia.org/wikipedia/commons/9/93/TesseractLogo.png\"><em>Tesseract-ocr logo</em></a></li>\n<li><a href=\"https://ocr.space/Content/Images/ocr.space.logo.png\"><em>OCR Space logo</em></a></li>\n<li><a href=\"https://image.slidesharecdn.com/googlevisionapi-160618133018/95/google-vision-api-1-638.jpg?cb=1466256779\"><em>Google Cloud Vision logo</em></a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ff8c1cb4a2f0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/ocr-for-password-detection-in-video-frames-ff8c1cb4a2f0\">OCR for password detection in video frames</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/730/1*dixc3zIwRaOkzQS-BIngDA.jpeg\"></figure><p>By mistake, a password or an access token could be displayed on an <a href=\"https://en.wikipedia.org/wiki/Proof_of_concept\">proof of concept</a> (PoC), demo or course. If this session is recorded and uploaded to the internet, this could be a major security issue. It would be helpful to detect these issues before sharing those videos to the world.</p>\n<p>We realized, at <a href=\"https://www.beeva.com/\">BEEVA</a>, that we could do that by using an OCR software. So we decided to do a PoC focused on evaluating several OCR tools capabilities to extract text from video frames. If we could extract text with acceptable quality, could we identify any password?</p>\n<p>Based on a set of videos of different kind (in local filesystem or uploaded to YouTube) we have made several tests for our purpose. In this post, we’re going to show strengths, weaknesses, cost, development effort, etc. of each OCR software analyzed.</p>\n<h3>Tesseract OCR</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/626/1*_45jJkVSEcJZh1nhbMcFqA.png\"><figcaption>Tesseract-OCR logo</figcaption></figure><p><a href=\"https://github.com/tesseract-ocr/tesseract/wiki\">Tesseract</a> is a free OCR software sponsored by Google since 2006. It’s considered one of the most accurate open-source OCR engines available. We used this OCR as a local choice and with default parameters and models.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Open source and easy to use.</li>\n<li>Supports 100 languages.</li>\n<li>Free and available on Linux, MAC and Windows (with an unofficial installer).</li>\n<li>You can train or improve models.</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>Doesn’t support language recognition.</li>\n<li>Very poor image pre-processing.</li>\n<li>High software development cost.</li>\n</ul>\n<h3>OCR Space</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/188/1*vH4q_q0LzuB1RjI28Kfs8w.png\"><figcaption>OCR.Space logo</figcaption></figure><p><a href=\"https://ocr.space/\">OCR.space</a> is a service of <a href=\"https://a9t9.com/about\">a9t9 software GmbH</a>. Provides a free to use, no registration necessary API with some size/number or requests limitations. Also, they offer an improved service with several <a href=\"https://ocr.space/ocrapi#free\">price plans</a>.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Easy to use, OS independent and well documented.</li>\n<li>Supports 24 languages.</li>\n<li>Provides a limited free OCR API.</li>\n<li>They assure you they are not keeping any of your data.</li>\n<li>Very low software development cost.</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>No chance to train or improve models.</li>\n<li>Doesn’t support language recognition.</li>\n<li>Unless you have a PRO PDF plan, you have a file size upload limit.</li>\n</ul>\n<h3>Google Cloud Vision</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/626/1*UCYXS1IiFi4eldwZJjq6QQ.jpeg\"><figcaption>Google Cloud Vision API logo</figcaption></figure><p><a href=\"https://cloud.google.com/vision\">Google Cloud Vision</a> API provides powerful machine learning models in an easy to use REST API. One point of the API is an OCR Service, with some <a href=\"https://cloud.google.com/vision/pricing\">free tier</a> capabilities but registration needed.</p>\n<p><strong>Pros</strong></p>\n<ul>\n<li>Easy to use, OS independent and well documented.</li>\n<li>Supports 56 languages.</li>\n<li>Provides a limited free OCR API.</li>\n<li>Supports language recognition.</li>\n<li>Very low software development cost.</li>\n<li>Models are trained and improved periodically by Google and their huge development team and computing capabilities.</li>\n</ul>\n<p><strong>Cons</strong></p>\n<ul>\n<li>No chance to train or improve models (not a real problem, as I said before, you have Google on your back improving their models).</li>\n<li>You have a file size upload limit to 4MB per image.</li>\n</ul>\n<h3>Summary report</h3>\n<p>In this last section, we’re going to tell you some general conclusions that we have made about this PoC and some hints for choosing one OCR software according to several criteria.</p>\n<h4>General insights</h4>\n<ul>\n<li>Frames extraction time cost will be linear for same video quality.</li>\n<li>\n<strong>Frame size</strong> (not resolution) will <strong>increase</strong> the <strong>cost</strong> dramatically.</li>\n<li>Tesseract (with default configuration) takes the <strong>same average time</strong> as <strong>OCR-Space or Google Cloud Vision </strong>(including networking time).</li>\n<li>\n<strong>Noise</strong> generated by OCR<strong> </strong>could <strong>spoil password recognition </strong>(since we use regexp patterns for identifying them).</li>\n</ul>\n<h4>Pricing</h4>\n<ul>\n<li>\n<strong>Tesseract</strong>: completely <strong>free</strong>.</li>\n<li>\n<strong>OCR.Space</strong>: free tier with 25K images (1MB image max. size). <strong>PRO plan</strong> with 250K images (5MB image max. size) <strong>24.95$/month</strong>. <strong>PRO PDF plan</strong> with 250K images (no size limit) <strong>49.95$/month</strong>. In both PRO plans, you can buy additional images upload by 10$/month each 100K additonal images.</li>\n<li>\n<strong>Google Cloud Vision</strong>: free tier with 1K first images. <strong>1.5$/month per 1K units</strong> from 1001 to 5M units and <strong>0.6$/month per 1K units</strong> from 5M to 20M units.</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/579/1*tdVBbqDn0buenQ1-14Tp1Q.png\"><figcaption>Pricing comparation between Tesseract, OCR.Space and Google Cloud Vision</figcaption></figure><p><strong>Frame size limitations</strong></p>\n<ul>\n<li>\n<strong>Tesseract</strong>: <strong>32767x32767 px</strong> image, <strong>no </strong>file <strong>size limitations</strong>.</li>\n<li>\n<strong>OCR Space</strong>: 1MB/image in free tier, 5MB/image in PRO plan and <strong>no limitations in PRO PDF plan</strong>.</li>\n<li>\n<strong>Google Cloud Vision</strong>: <strong>4MB/image </strong>and a maximum of <strong>8MB/request </strong>(allowing to upload 16 images in a single request) in all billing types.</li>\n</ul>\n<p><strong>Frame number limitations</strong></p>\n<ul>\n<li>\n<strong>Tesseract</strong>: <strong>no limitations</strong>, limited only by local storage.</li>\n<li>\n<strong>OCR Space</strong>: 25K req/month and 500 calls/day in free tier<strong> </strong>and <strong>250K req/month</strong> and <strong>600 calls/min</strong> in all PRO plans (but you can buy additional loads in 100K image blocks)</li>\n<li>\n<strong>Google Cloud Vision</strong>: <strong>600 calls/min</strong>, <strong>20M images/month </strong>and 16 images/request in all billing types.</li>\n</ul>\n<p><strong>Development</strong></p>\n<ul>\n<li>\n<strong>Tesseract</strong>: You may need a <strong>strong effort</strong>, or a very good quality images, for achieving quite good results, but <strong>you can train and improve</strong> your own or previous existing <strong>models</strong> as you wish.</li>\n<li>\n<strong>OCR Space </strong>and <strong>Google Cloud Vision</strong>: very <strong>low effort</strong>. You only have to make an HTTP request module or adapter in the worst case.</li>\n</ul>\n<p><strong>Accuracy</strong></p>\n<ul>\n<li>\n<strong>Tesseract</strong>: <strong>very poor </strong>without training and only with <strong>one language texts</strong>.</li>\n<li>\n<strong>OCR Space</strong>: good or <strong>acceptable</strong> for only <strong>one language texts</strong>.</li>\n<li>\n<strong>Google Cloud Vision</strong>: <strong>awesome! </strong>and<strong> needed </strong>in<strong> multi language texts</strong>.</li>\n</ul>\n<h3>Final thoughts</h3>\n<p><strong>Tesseract</strong>, without a quite good images and without spending a bunch of development time, <strong>may be</strong> a little bit <strong>discouraged</strong>, but <strong>necessary</strong> in some projects where you have to use only your local storage <strong>due to legal clauses</strong>.</p>\n<p>On the other hand, and if you have a <strong>multilanguage</strong> text or a <strong>poor</strong> image <strong>quality</strong>, I would choose <strong>Google Cloud Vision</strong> without any doubts. <strong>OCR Space </strong>would be an interesting and <strong>cheaper</strong> option but only if you have a good enough frameset and mostly in a <strong>unique language</strong>.</p>\n<h4><em>Links</em></h4>\n<ul>\n<li><a href=\"https://docs.google.com/presentation/d/1pa1BLJcjipi5h6o4_QPFR_aF-83I03CfgTXqb56dWtE\"><em>OCR for password detection in video frames presentation</em></a></li>\n<li><a href=\"https://github.com/beeva-luismesa/beeva-poc-ocr\"><em>This PoC repository</em></a></li>\n<li>\n<a href=\"http://projectnaptha.com/\"><em>Project Naptha</em></a><em> (a Google Chrome extension to use Tesseract on images displayed on your browser)</em>\n</li>\n<li>\n<a href=\"https://chrome.google.com/webstore/detail/copyfish-%F0%9F%90%9F-free-ocr-soft/eenjdnjldapjajjofmldgmkjaienebbj\"><em>Copyfish for Google Chrome</em></a><em> (extension to use OCR.Space on images or videos displayed on your browser)</em>\n</li>\n<li>\n<a href=\"https://addons.mozilla.org/es/firefox/addon/copyfish-ocr-software/\"><em>Copyfish for Mozilla Firefox</em></a><em> (addon to use OCR.Space on images or videos displayed on your browser)</em>\n</li>\n<li><a href=\"https://cloud.google.com/vision/docs/drag-and-drop\"><em>Google Cloud Vision ‘drag and drop’ demo</em></a></li>\n</ul>\n<h4><em>Images</em></h4>\n<ul>\n<li><a href=\"https://pixabay.com/es/hacker-de-seguridad-cyber-internet-2002907/\"><em>Main picture</em></a></li>\n<li><a href=\"https://upload.wikimedia.org/wikipedia/commons/9/93/TesseractLogo.png\"><em>Tesseract-ocr logo</em></a></li>\n<li><a href=\"https://ocr.space/Content/Images/ocr.space.logo.png\"><em>OCR Space logo</em></a></li>\n<li><a href=\"https://image.slidesharecdn.com/googlevisionapi-160618133018/95/google-vision-api-1-638.jpg?cb=1466256779\"><em>Google Cloud Vision logo</em></a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ff8c1cb4a2f0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/ocr-for-password-detection-in-video-frames-ff8c1cb4a2f0\">OCR for password detection in video frames</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "research",
            "google-cloud-platform",
            "ocr-space",
            "ocr",
            "tesseract"
        ],
        "area": "insigths",
        "active": true,
        "id": "_q3hhq3m5y"
    },
    {
        "title": "Training Deep Learning Models on Multi-GPUs",
        "pubDate": "2017-10-30 11:59:26",
        "link": "https://labs.beeva.com/training-deep-learning-models-on-multi-gpus-a3cb7ca07e97?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/a3cb7ca07e97",
        "author": "Enrique Otero BEEVA",
        "thumbnail": "https://cdn-images-1.medium.com/max/320/0*p-U9WRxzcvu1TrPG.gif",
        "description": "\n<p><strong>Deep Learning disruption</strong> is providing amazing results on several Machine Learning fields: Computer Vision, Speech Recognition, Machine Translation or Ranking problems. The latter applies to Recommender Systems that focus on <a href=\"http://technocalifornia.blogspot.com.es/2013/07/recommendations-as-personalized.html\">learning to rank</a> instead of predicting ratings.</p>\n<p>The fundamentals of current Deep Learning techniques were known for decades. Artificial Neural Networks come from 60s, Backpropagation has 30 years old, and 7-layer LeNet Convolutional Neuronal Network was presented on 1998 by <a href=\"https://research.fb.com/people/lecun-yann/\">Yann Le Cunn</a>, current director of AI Research at Facebook. However Deep Learning recent success is mainly due to 3 reasons: <strong>more </strong>(labeled)<strong> data, more computing power</strong>, and some tricks.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/320/0*p-U9WRxzcvu1TrPG.gif\"><figcaption>Source: <a href=\"http://yann.lecun.com/exdb/lenet/\">http://yann.lecun.com/exdb/lenet/</a></figcaption></figure><p>Actually Deep Learning explosion is <strong>powered by GPUs</strong>! And in terms of accelerating neural networks, GPU is synonym of <strong>NVIDIA</strong> and <a href=\"https://developer.nvidia.com/about-cuda\"><strong>CUDA</strong></a>. Curiously, GPUs were originally powered by video gamers. And some of the last advances in Artificial Intelligence are also related to video games.</p>\n<h3>The challenge</h3>\n<p>Training 90 epochs of ResNet-50 model on ImageNet dataset can take approximately a week on a single GPU. Even with an optimized implementation and a pretty good GPU like Pascal Titan X it takes more than 3 days. And bigger models with bigger datasets usually get better results. This emphasizes the necessity to <strong>accelerate training</strong> of Deep Learning models. But this is a real challenge!</p>\n<p>At <a href=\"https://www.beeva.com/\">BEEVA</a> Labs we performed some internal benchmarks testing several <strong>data-parallel</strong> implementations of well-known image classification models. Mainly LeNet, AlexNet and ResNet50. Our scenario included MNIST and CIFAR10 datasets, also well-known though much smaller than ImageNet. And implementations on Tensorflow, Keras+Tensorflow and MxNet. Detailed conclusions of our benchmark will excede the limits of a 5 mins post. So I’ll summarize our first experiences in this topic.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/731/1*zcVYESnH3Ogg0T76lbfw5g.png\"></figure><p>We performed some tests on a p2.8x instance with 8 NVIDIA Tesla K80 on AWS. And we share <a href=\"https://github.com/beeva-enriqueotero/beeva-poc-mxnet/blob/master/README_cifar.m\">our results</a> on MxNet CIFAR10 image classification with Resnet50 model. Following a synchronous data-parallel Stochastic Gradient Descent implementation configured on mini-batches of 256 samples per GPU. Fortunately <a href=\"https://github.com/apache/incubator-mxnet/tree/master/example/image-classification\">MxNet example</a>s make easy to achieve this goal.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*IV3C95um1YgoovnUnhLgoQ.png\"></figure><p>According to the previous benchmark, p2.8x scaled near linearly with the number of GPUs. With 90% efficiency in scaling up from 1 to 8 GPUs.</p>\n<p>Otherwise, we realized larger batch sizes would be required in order to completely saturate the 8 K80 GPUs. But on expenses of non-trivial convergence difficulties we should deal to avoid accuracy degradation.</p>\n<h3>Our conclusions</h3>\n<p>Some general recommendations can be made to efficiently scale training of Deep Learning models:</p>\n<ul>\n<li>\n<strong>Dataset </strong>matters: first of all Cifar10 images appears to be a <strong>too small dataset </strong>to use for benchmarking. As computation required for processing each sample is minimal. And it makes difficult to adequately saturate the GPU utilization. So it’s hard to exploit the possibilities of modern GPUs with such small dataset.</li>\n<li>\n<strong>Data pipeline </strong>matters: even the way to get data from hard disk can be a bottleneck too. Or issues related with image preprocessing. So you probably need Solid State Disks (SSD) and an efficient pipeline to feed your model.</li>\n<li>\n<strong>Model</strong> matters: usually bigger models increase both computation cost and network bandwith requirements. However, the correlation is not the same for all deep learning models. For instance, <em>Alexnet</em> requires a relatively small number of operations for a single forward pass. But it has a huge number of network parameters to share. So it’s specially bandwith demanding.</li>\n<li>\n<strong>Topology </strong>matters: for big models, with modern GPUs, <strong>bandwith </strong>of interconnections is actually the main bottleneck. Even within a single node there is a big difference between PCIe and recent NVlink interconnections. And going multinode latencies get much worse, so Infiniband or very high speed Ethernet connections (&gt;25Gbit/s) are needed.</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/840/0*jV731nxhlu1Eq-gt.png\"><figcaption>Source: <a href=\"http://www.nvidia.com/object/nvlink.html\">http://www.nvidia.com/object/nvlink.html</a></figcaption></figure><ul><li>\n<strong>Batch size</strong> and <strong>learning rates</strong> matter. In order to reduce communication overhead and effectively use all the computing parallelism GPUs offer, we require large batch sizes. But large batch sizes cause convergence difficulties to Stochastic Gradient Descent degrading accuracy. <a href=\"https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/\">Recent paper from Facebook AI Research</a> shows it’s possible to practically circumvent this situation by progressively increasing learning rates in a linear scale. Appart from many other considerations.</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/724/0*HT4D4WT9-jze2Cvc.png\"><figcaption>Source: <a href=\"https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/\">https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/</a></figcaption></figure><p>Besides, other implementation details also matter. And we will go deep on this topic on future posts. As devil is in details. Anyhow, all benchmarks are wrong, some are useful. And this post is going to exceed the barrier of 5 mins reading ;)</p>\n<h4>Acknowledgement:</h4>\n<blockquote>We have to thank Rafael and Francisco from <a href=\"http://www.azken.com/\"><strong>Azken Muga</strong></a>. Thanks to NVIDIA engineers for providing us valuable feedback about this post and more tips about benchmarking and efficiently training deep neural networks. Thanks to Gorka from <a href=\"https://ailoveu.es/\"><strong>Ailoveu</strong></a> . Of course, thanks to <strong>BEEVA</strong> for their support to all this cool stuff. And thanks to my colleagues Ricardo and Jorge.</blockquote>\n<p><em>Source featured image: </em><a href=\"http://www.hardwarezone.com.my/tech-news-computex-2016-nvidia-s-pascal-powers-vr-deep-learning-and-gaming\"><em>http://www.hardwarezone.com.my/tech-news-computex-2016-nvidia-s-pascal-powers-vr-deep-learning-and-gaming</em></a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3cb7ca07e97\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/training-deep-learning-models-on-multi-gpus-a3cb7ca07e97\">Training Deep Learning Models on Multi-GPUs</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<p><strong>Deep Learning disruption</strong> is providing amazing results on several Machine Learning fields: Computer Vision, Speech Recognition, Machine Translation or Ranking problems. The latter applies to Recommender Systems that focus on <a href=\"http://technocalifornia.blogspot.com.es/2013/07/recommendations-as-personalized.html\">learning to rank</a> instead of predicting ratings.</p>\n<p>The fundamentals of current Deep Learning techniques were known for decades. Artificial Neural Networks come from 60s, Backpropagation has 30 years old, and 7-layer LeNet Convolutional Neuronal Network was presented on 1998 by <a href=\"https://research.fb.com/people/lecun-yann/\">Yann Le Cunn</a>, current director of AI Research at Facebook. However Deep Learning recent success is mainly due to 3 reasons: <strong>more </strong>(labeled)<strong> data, more computing power</strong>, and some tricks.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/320/0*p-U9WRxzcvu1TrPG.gif\"><figcaption>Source: <a href=\"http://yann.lecun.com/exdb/lenet/\">http://yann.lecun.com/exdb/lenet/</a></figcaption></figure><p>Actually Deep Learning explosion is <strong>powered by GPUs</strong>! And in terms of accelerating neural networks, GPU is synonym of <strong>NVIDIA</strong> and <a href=\"https://developer.nvidia.com/about-cuda\"><strong>CUDA</strong></a>. Curiously, GPUs were originally powered by video gamers. And some of the last advances in Artificial Intelligence are also related to video games.</p>\n<h3>The challenge</h3>\n<p>Training 90 epochs of ResNet-50 model on ImageNet dataset can take approximately a week on a single GPU. Even with an optimized implementation and a pretty good GPU like Pascal Titan X it takes more than 3 days. And bigger models with bigger datasets usually get better results. This emphasizes the necessity to <strong>accelerate training</strong> of Deep Learning models. But this is a real challenge!</p>\n<p>At <a href=\"https://www.beeva.com/\">BEEVA</a> Labs we performed some internal benchmarks testing several <strong>data-parallel</strong> implementations of well-known image classification models. Mainly LeNet, AlexNet and ResNet50. Our scenario included MNIST and CIFAR10 datasets, also well-known though much smaller than ImageNet. And implementations on Tensorflow, Keras+Tensorflow and MxNet. Detailed conclusions of our benchmark will excede the limits of a 5 mins post. So I’ll summarize our first experiences in this topic.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/731/1*zcVYESnH3Ogg0T76lbfw5g.png\"></figure><p>We performed some tests on a p2.8x instance with 8 NVIDIA Tesla K80 on AWS. And we share <a href=\"https://github.com/beeva-enriqueotero/beeva-poc-mxnet/blob/master/README_cifar.m\">our results</a> on MxNet CIFAR10 image classification with Resnet50 model. Following a synchronous data-parallel Stochastic Gradient Descent implementation configured on mini-batches of 256 samples per GPU. Fortunately <a href=\"https://github.com/apache/incubator-mxnet/tree/master/example/image-classification\">MxNet example</a>s make easy to achieve this goal.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*IV3C95um1YgoovnUnhLgoQ.png\"></figure><p>According to the previous benchmark, p2.8x scaled near linearly with the number of GPUs. With 90% efficiency in scaling up from 1 to 8 GPUs.</p>\n<p>Otherwise, we realized larger batch sizes would be required in order to completely saturate the 8 K80 GPUs. But on expenses of non-trivial convergence difficulties we should deal to avoid accuracy degradation.</p>\n<h3>Our conclusions</h3>\n<p>Some general recommendations can be made to efficiently scale training of Deep Learning models:</p>\n<ul>\n<li>\n<strong>Dataset </strong>matters: first of all Cifar10 images appears to be a <strong>too small dataset </strong>to use for benchmarking. As computation required for processing each sample is minimal. And it makes difficult to adequately saturate the GPU utilization. So it’s hard to exploit the possibilities of modern GPUs with such small dataset.</li>\n<li>\n<strong>Data pipeline </strong>matters: even the way to get data from hard disk can be a bottleneck too. Or issues related with image preprocessing. So you probably need Solid State Disks (SSD) and an efficient pipeline to feed your model.</li>\n<li>\n<strong>Model</strong> matters: usually bigger models increase both computation cost and network bandwith requirements. However, the correlation is not the same for all deep learning models. For instance, <em>Alexnet</em> requires a relatively small number of operations for a single forward pass. But it has a huge number of network parameters to share. So it’s specially bandwith demanding.</li>\n<li>\n<strong>Topology </strong>matters: for big models, with modern GPUs, <strong>bandwith </strong>of interconnections is actually the main bottleneck. Even within a single node there is a big difference between PCIe and recent NVlink interconnections. And going multinode latencies get much worse, so Infiniband or very high speed Ethernet connections (&gt;25Gbit/s) are needed.</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/840/0*jV731nxhlu1Eq-gt.png\"><figcaption>Source: <a href=\"http://www.nvidia.com/object/nvlink.html\">http://www.nvidia.com/object/nvlink.html</a></figcaption></figure><ul><li>\n<strong>Batch size</strong> and <strong>learning rates</strong> matter. In order to reduce communication overhead and effectively use all the computing parallelism GPUs offer, we require large batch sizes. But large batch sizes cause convergence difficulties to Stochastic Gradient Descent degrading accuracy. <a href=\"https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/\">Recent paper from Facebook AI Research</a> shows it’s possible to practically circumvent this situation by progressively increasing learning rates in a linear scale. Appart from many other considerations.</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/724/0*HT4D4WT9-jze2Cvc.png\"><figcaption>Source: <a href=\"https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/\">https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/</a></figcaption></figure><p>Besides, other implementation details also matter. And we will go deep on this topic on future posts. As devil is in details. Anyhow, all benchmarks are wrong, some are useful. And this post is going to exceed the barrier of 5 mins reading ;)</p>\n<h4>Acknowledgement:</h4>\n<blockquote>We have to thank Rafael and Francisco from <a href=\"http://www.azken.com/\"><strong>Azken Muga</strong></a>. Thanks to NVIDIA engineers for providing us valuable feedback about this post and more tips about benchmarking and efficiently training deep neural networks. Thanks to Gorka from <a href=\"https://ailoveu.es/\"><strong>Ailoveu</strong></a> . Of course, thanks to <strong>BEEVA</strong> for their support to all this cool stuff. And thanks to my colleagues Ricardo and Jorge.</blockquote>\n<p><em>Source featured image: </em><a href=\"http://www.hardwarezone.com.my/tech-news-computex-2016-nvidia-s-pascal-powers-vr-deep-learning-and-gaming\"><em>http://www.hardwarezone.com.my/tech-news-computex-2016-nvidia-s-pascal-powers-vr-deep-learning-and-gaming</em></a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3cb7ca07e97\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/training-deep-learning-models-on-multi-gpus-a3cb7ca07e97\">Training Deep Learning Models on Multi-GPUs</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "deep-learning",
            "gpu",
            "knowledge",
            "beeva-labs",
            "machine-intelligence"
        ],
        "area": "insigths",
        "active": true,
        "id": "_v1xm66psc"
    },
    {
        "title": "Spatial Apps y Dimensional UI",
        "pubDate": "2017-09-18 10:37:04",
        "link": "https://labs.beeva.com/spatial-apps-y-dimensional-ui-b7eea3e5148?source=rss----cb529f64e388---4",
        "guid": "https://medium.com/p/b7eea3e5148",
        "author": "Jorge Andrés Martínez | BEEVA",
        "thumbnail": "https://cdn-images-1.medium.com/max/1024/1*46Bwq9MFdCRUMXOMfp1fjw.jpeg",
        "description": "\n<h4>El futuro del diseño de interfaz de usuario</h4>\n<p>Con la apuesta definitiva de <strong>Apple</strong>, <strong>Google</strong> y <strong>Microsoft </strong>por el mundo de la <strong>Realidad Aumentada </strong>y <strong>Mixta</strong> parece inevitable pensar que en un futuro no muy lejano dejaremos de interactuar con las pantallas de nuestros teléfonos móviles y tablets para empezar a interactuar con un <strong>interfaz holográfico</strong> integrado en nuestro mundo.</p>\n<p><a href=\"https://developer.microsoft.com/en-us/windows/projects/campaigns/windows-mixed-reality\"><strong>Microsof</strong></a><strong>t</strong> ya dio un primer paso con las <a href=\"https://developer.microsoft.com/es-es/windows/mixed-reality\"><strong>Hololens</strong></a> y ahora ha subido la apuesta con una gama de cascos de Realidad Mixta compatibles con <strong>Windows 10</strong> y su nuevo sistema de diseño<a href=\"https://fluent.microsoft.com/\"> <strong>Fluent Design</strong></a><strong>.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*46Bwq9MFdCRUMXOMfp1fjw.jpeg\"><figcaption>Cascos Windows 10 Mixed Reality disponibles está Navidad de la mano de Acer, Dell , HP , Lenovo y Asus</figcaption></figure><p>Por su parte <strong>Apple</strong> con el <strong>ARkit</strong> y los nuevos modelos de <strong>iphone 8 – iphone X </strong>preparados específicamente para <strong>AR</strong> y <strong>Google</strong> con <a href=\"https://developers.google.com/ar/\"><strong>ARcore</strong></a> que pronto llegará a miles de dispositivos Android, han dado un gran paso y un empujón a la Realidad Aumentada y Realidad Mixta.</p>\n<p><em>Un pequeño apunte…</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4VikvJhvcYSUAU5M40vJuA.jpeg\"><figcaption>Realidad Aumentada ≠ Realidad Mixta</figcaption></figure><blockquote>En Realidad Aumentada los elementos aumentados están superpuestos mientras que en la Realidad Mixta los objetos físicos y virtuales coexisten e interactúan en tiempo real.</blockquote>\n<h3>Spatial apps</h3>\n<p>Si nos imaginamos un interfaz de usuario, compuesto por elementos aumentados en un entorno cualquiera con los que podemos interactuar desde un móvil o unas gafas de <strong>Realidad Mixta</strong> puede parecer futurista, complejo o intrusivo, pero ateniéndonos a la evolución exponencial de la tecnología y comprobando el <strong><em>bluf</em> de la primera generación de wearables </strong>(a no ser que te guste contarte los pasos) no es raro pensar en un <strong>2020</strong> repleto de información aumentada en espacios abiertos y en usuarios con <strong>wearables de segunda o tercera generación</strong> en sus cabezas.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/750/1*XSKi-TtuQVOgIHUjGpEFsQ.png\"><figcaption>Ejemplo de interfaz con elementos 2d aumentados (UX Mobile AR FRAMEWORK BEEVA)</figcaption></figure><p>En las Spatial apps, los <strong>elementos 2D aumentados a modo de señalética</strong> nos ampliarán información o nos ayudarán a tomar decisiones de forma personalizada gracias a nuestros móviles y a dispositivos como gafas de <strong>Realidad Mixta</strong> o <strong>LentillARs</strong> en el futuro.</p>\n<p><strong>Los interfaces serán contextuales, según el espacio en el que nos encontremos, muy parecido a la señalética actual</strong> y los diseñadores tendremos que adaptarnos a este nuevo sistema de diseño inmersivo para aprovechar al máximo sus posibilidades.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*Xl4MEUPxrg5KdgKw8sOFJQ.png\"><figcaption>Ejemplo de interfaz con elementos 2d integrados en el espacio (UX Mobile AR FRAMEWORK de <a href=\"https://www.beeva.com/\">BEEVA</a>)</figcaption></figure><h3>Dimensional UI</h3>\n<p>El <strong>UI dimensional</strong> permitirá explorar al máximo las posibilidades de los entornos inmersivos, <strong>eliminará las barreras de los dispositivos 2d</strong> tradicionales, <strong>permitiendo una navegación ambiental</strong> más natural y <strong>con menos carga cognitiva, invitando a la interacción</strong> y <strong>mejorando el sentido de presencia.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*InS0509NT5QaJSiaHp-i8A.png\"><figcaption>Ejemplo Dimensional UI</figcaption></figure><p>Es cierto que para este gran paso <strong>tendremos que esperar algún tiempo</strong>, ya que las posibilidades se multiplicarán cuando las gafas de <strong>Realidad Mixta </strong>sean portables y nos permitan experimentar ese nuevo interfaz dimensional desde cualquier entorno.</p>\n<h3>Conclusiones</h3>\n<p><em>Esto son solo algunas reflexiones sobre el futuro del diseño UI, surgidas a raíz de experimentar con VR/AR en </em><a href=\"https://labs.beeva.com/\"><em>BEEVA labs</em></a></p>\n<p>En nuestros experimentos hemos llegado a algunas conclusiones que estamos recopilando en un<strong> </strong><a href=\"https://beeva-labs.github.io/ux-mobile-ar-framework/#inusuarios\"><strong>Framework de AR</strong></a> donde vamos a tratar la Realidad Aumentada desde el punto de vista de la experiencia de usuario y el diseño de interacción.</p>\n<p>Podéis echar un vistazo y comentarnos que os parece, creemos importante que todos hablemos el mismo idioma, que cuando hagamos referencia a términos todos pensemos en los mismo y de paso <strong>intentemos crear un estándar dentro de un sector que está empezando a crecer exponencialmente.</strong></p>\n<p><em>Cualquier feedback será bien recibido.</em></p>\n<h4><strong>Relaccionados:</strong></h4>\n<p><a href=\"https://beeva-labs.github.io/ux-mobile-ar-framework/#inusuarios\">UX Mobile AR FRAMEWORK</a></p>\n<p><a href=\"https://beeva-sergiosantamaria.github.io/arKitDoc/\"><em>Apple ARkit Documentación</em></a><em> by </em><a href=\"https://medium.com/u/f34c3e8e516a\"><em>Sergio Santamaria</em></a></p>\n<p><a href=\"https://developer.apple.com/ios/human-interface-guidelines/technologies/augmented-reality/\"><em>iOS AR Human Interface Guidelines</em></a></p>\n<p><a href=\"https://fluent.microsoft.com/\"><em>Microsoft Fluent Design system</em></a></p>\n<p><a href=\"https://medium.com/microsoft-design/from-3d-to-2d-and-back-again-479906df6b47\">From 3D to 2D and back again</a></p>\n<h4>Menos Relaccionados:</h4>\n<p><a href=\"https://standardsmanual.com/products/nyctacompactedition\">1970 New York City Transit Authority Graphics Standards Manual</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=Bec_WaunPMo\">Documental Helvetica Subtitulado Español</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b7eea3e5148\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/spatial-apps-y-dimensional-ui-b7eea3e5148\">Spatial Apps y Dimensional UI</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "content": "\n<h4>El futuro del diseño de interfaz de usuario</h4>\n<p>Con la apuesta definitiva de <strong>Apple</strong>, <strong>Google</strong> y <strong>Microsoft </strong>por el mundo de la <strong>Realidad Aumentada </strong>y <strong>Mixta</strong> parece inevitable pensar que en un futuro no muy lejano dejaremos de interactuar con las pantallas de nuestros teléfonos móviles y tablets para empezar a interactuar con un <strong>interfaz holográfico</strong> integrado en nuestro mundo.</p>\n<p><a href=\"https://developer.microsoft.com/en-us/windows/projects/campaigns/windows-mixed-reality\"><strong>Microsof</strong></a><strong>t</strong> ya dio un primer paso con las <a href=\"https://developer.microsoft.com/es-es/windows/mixed-reality\"><strong>Hololens</strong></a> y ahora ha subido la apuesta con una gama de cascos de Realidad Mixta compatibles con <strong>Windows 10</strong> y su nuevo sistema de diseño<a href=\"https://fluent.microsoft.com/\"> <strong>Fluent Design</strong></a><strong>.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*46Bwq9MFdCRUMXOMfp1fjw.jpeg\"><figcaption>Cascos Windows 10 Mixed Reality disponibles está Navidad de la mano de Acer, Dell , HP , Lenovo y Asus</figcaption></figure><p>Por su parte <strong>Apple</strong> con el <strong>ARkit</strong> y los nuevos modelos de <strong>iphone 8 – iphone X </strong>preparados específicamente para <strong>AR</strong> y <strong>Google</strong> con <a href=\"https://developers.google.com/ar/\"><strong>ARcore</strong></a> que pronto llegará a miles de dispositivos Android, han dado un gran paso y un empujón a la Realidad Aumentada y Realidad Mixta.</p>\n<p><em>Un pequeño apunte…</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4VikvJhvcYSUAU5M40vJuA.jpeg\"><figcaption>Realidad Aumentada ≠ Realidad Mixta</figcaption></figure><blockquote>En Realidad Aumentada los elementos aumentados están superpuestos mientras que en la Realidad Mixta los objetos físicos y virtuales coexisten e interactúan en tiempo real.</blockquote>\n<h3>Spatial apps</h3>\n<p>Si nos imaginamos un interfaz de usuario, compuesto por elementos aumentados en un entorno cualquiera con los que podemos interactuar desde un móvil o unas gafas de <strong>Realidad Mixta</strong> puede parecer futurista, complejo o intrusivo, pero ateniéndonos a la evolución exponencial de la tecnología y comprobando el <strong><em>bluf</em> de la primera generación de wearables </strong>(a no ser que te guste contarte los pasos) no es raro pensar en un <strong>2020</strong> repleto de información aumentada en espacios abiertos y en usuarios con <strong>wearables de segunda o tercera generación</strong> en sus cabezas.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/750/1*XSKi-TtuQVOgIHUjGpEFsQ.png\"><figcaption>Ejemplo de interfaz con elementos 2d aumentados (UX Mobile AR FRAMEWORK BEEVA)</figcaption></figure><p>En las Spatial apps, los <strong>elementos 2D aumentados a modo de señalética</strong> nos ampliarán información o nos ayudarán a tomar decisiones de forma personalizada gracias a nuestros móviles y a dispositivos como gafas de <strong>Realidad Mixta</strong> o <strong>LentillARs</strong> en el futuro.</p>\n<p><strong>Los interfaces serán contextuales, según el espacio en el que nos encontremos, muy parecido a la señalética actual</strong> y los diseñadores tendremos que adaptarnos a este nuevo sistema de diseño inmersivo para aprovechar al máximo sus posibilidades.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*Xl4MEUPxrg5KdgKw8sOFJQ.png\"><figcaption>Ejemplo de interfaz con elementos 2d integrados en el espacio (UX Mobile AR FRAMEWORK de <a href=\"https://www.beeva.com/\">BEEVA</a>)</figcaption></figure><h3>Dimensional UI</h3>\n<p>El <strong>UI dimensional</strong> permitirá explorar al máximo las posibilidades de los entornos inmersivos, <strong>eliminará las barreras de los dispositivos 2d</strong> tradicionales, <strong>permitiendo una navegación ambiental</strong> más natural y <strong>con menos carga cognitiva, invitando a la interacción</strong> y <strong>mejorando el sentido de presencia.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*InS0509NT5QaJSiaHp-i8A.png\"><figcaption>Ejemplo Dimensional UI</figcaption></figure><p>Es cierto que para este gran paso <strong>tendremos que esperar algún tiempo</strong>, ya que las posibilidades se multiplicarán cuando las gafas de <strong>Realidad Mixta </strong>sean portables y nos permitan experimentar ese nuevo interfaz dimensional desde cualquier entorno.</p>\n<h3>Conclusiones</h3>\n<p><em>Esto son solo algunas reflexiones sobre el futuro del diseño UI, surgidas a raíz de experimentar con VR/AR en </em><a href=\"https://labs.beeva.com/\"><em>BEEVA labs</em></a></p>\n<p>En nuestros experimentos hemos llegado a algunas conclusiones que estamos recopilando en un<strong> </strong><a href=\"https://beeva-labs.github.io/ux-mobile-ar-framework/#inusuarios\"><strong>Framework de AR</strong></a> donde vamos a tratar la Realidad Aumentada desde el punto de vista de la experiencia de usuario y el diseño de interacción.</p>\n<p>Podéis echar un vistazo y comentarnos que os parece, creemos importante que todos hablemos el mismo idioma, que cuando hagamos referencia a términos todos pensemos en los mismo y de paso <strong>intentemos crear un estándar dentro de un sector que está empezando a crecer exponencialmente.</strong></p>\n<p><em>Cualquier feedback será bien recibido.</em></p>\n<h4><strong>Relaccionados:</strong></h4>\n<p><a href=\"https://beeva-labs.github.io/ux-mobile-ar-framework/#inusuarios\">UX Mobile AR FRAMEWORK</a></p>\n<p><a href=\"https://beeva-sergiosantamaria.github.io/arKitDoc/\"><em>Apple ARkit Documentación</em></a><em> by </em><a href=\"https://medium.com/u/f34c3e8e516a\"><em>Sergio Santamaria</em></a></p>\n<p><a href=\"https://developer.apple.com/ios/human-interface-guidelines/technologies/augmented-reality/\"><em>iOS AR Human Interface Guidelines</em></a></p>\n<p><a href=\"https://fluent.microsoft.com/\"><em>Microsoft Fluent Design system</em></a></p>\n<p><a href=\"https://medium.com/microsoft-design/from-3d-to-2d-and-back-again-479906df6b47\">From 3D to 2D and back again</a></p>\n<h4>Menos Relaccionados:</h4>\n<p><a href=\"https://standardsmanual.com/products/nyctacompactedition\">1970 New York City Transit Authority Graphics Standards Manual</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=Bec_WaunPMo\">Documental Helvetica Subtitulado Español</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b7eea3e5148\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://labs.beeva.com/spatial-apps-y-dimensional-ui-b7eea3e5148\">Spatial Apps y Dimensional UI</a> was originally published in <a href=\"https://labs.beeva.com/\">BEEVA Labs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "augmented-reality",
            "interaction-design",
            "ui",
            "virtual-reality",
            "mixed-reality"
        ],
        "area": "insigths",
        "active": true,
        "id": "_8nzwiw413"
    }
]